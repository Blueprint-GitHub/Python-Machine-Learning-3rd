{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPSnmOdx0YsSQytC36IseTi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["Tip 1 혼합 정밀도"],"metadata":{"id":"gUPsJjUzwRgw"}},{"cell_type":"code","source":["\"\"\"혼합 정밀도 (float16 + float32)로 모델 훈련하여 최적화 하기\n","일반적으로 모델의 정방향패스는 float16으로 진행되고 역전파(가중치)는\n","float32로 업데이트 된다. 특정 층에서 혼합 정밀도를 설정하지 않으려면\n","dtype = 'float32' 처럼 매개변수를 전달하면 되며\n","크로스엔트로피나 소프트맥스는 float16에서 수치적으로 불안정하므로\n","직접 매개변수를 전달해주는 것이 좋다.\"\"\"\n","\n","# crossentropy, softmax / dtype = 'float32'\n","from tensorflow import keras\n","keras.mixed_precision.set_global_policy('mixed_float16')\n","\n","\"\"\"\n","crossentropy는 사용자 정의 손실 함수를 만들어 사용해야 한다.\n","# 레이블이 0~1일 경우 from_logits = True 손실값은 모델마다 다르므로 신경 x\n","\n","import tensorflow as tf\n","\n","#간단한 버전\n","def custom_crossentropy(y_true, y_pred):\n","    y_true = tf.cast(y_true, dtype=tf.float32)  # Labels are cast to float32\n","    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)  # Calculate cross-entropy loss\n","    return loss\n","\n","\n","#다중 분류는 categorical_crossentropy지정.\n","class CustomBinaryCrossEntropy(tf.keras.losses.Loss):\n","    def __init__(self, from_logits=False, label_smoothing=0, name='binary_crossentropy'):\n","        super().__init__(name=name)\n","        self.from_logits = from_logits\n","        self.label_smoothing = label_smoothing\n","\n","    @tf.function\n","    def call(self, y_true, y_pred):\n","        y_true = tf.cast(y_true, tf.float32)\n","        y_pred = tf.cast(y_pred, tf.float32)\n","\n","        # 수치적 안정성을 위한 로직\n","        if self.from_logits:\n","            y_pred = tf.nn.sigmoid(y_pred)\n","\n","        return tf.reduce_mean(tf.keras.losses.binary_crossentropy(\n","            y_true, y_pred, label_smoothing=self.label_smoothing\n","        ))\n","-> model.compile(optimizer='adam', loss=CustomBinaryEntropy(), metrics=['accuracy'])\n","\n","#Sparse도 똑같이 만들면 됨.\n","class CustomSparseCategoricalCrossentropy(tf.keras.losses.Loss):\n","    def __init__(self, from_logits=False, name='sparse_categorical_crossentropy'):\n","        super().__init__(name=name)\n","        self.from_logits = from_logits\n","\n","    def call(self, y_true, y_pred):\n","        y_true = tf.cast(y_true, tf.int64)\n","        y_pred = tf.cast(y_pred, tf.float32)\n","        return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(\n","            y_true, y_pred, from_logits=self.from_logits\n","        ))\n","\"\"\""],"metadata":{"id":"Xdf7H5Xnuell"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tip 2 TPU 연산 사용"],"metadata":{"id":"9o76qaLrwU2P"}},{"cell_type":"code","source":["# 일단 전처리 ( 열려라 참깨 주문)\n","import tensorflow as tf\n","\n","tpu = tf.distribue.cluster_resolver.TPUClusterResolver.connect()\n","print(\"장치:\", tpu.master())"],"metadata":{"id":"Rn2FYo4FwWss"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" TPU를 사용하려면 다중 GPU처럼 with문으로 블록을 감싸야 한다.\n","이는 TPU 코어마다 모델이 복제되고 모든 모델은 동기화를 유지한다는 뜻이다.\"\"\"\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","strategy = tf.distribute.TPUstrategy(tpu)\n","print(f\"복제 모델 개수: {strategy.num_replicas_in_sync}\")\n","\n","def build_model(input_size):\n","    # input, x , outputs, model, model.compile까지 작성한다.\n","    return model\n","\n","with strategy.scope():\n","    model = build_model(input_size = 32)\n","\n","\"\"\" 코랩에서 TPU를 사용할경우 데이터를 로딩할떄 문제가 있는데,\n","넘파이 배열로 훈련한다면 상관이 없고 혹은 GCS(google cloud storage)버킷에 저장하고\n","버킷에서 바로 읽어들이는 데이터셋을 만들어야 한다. 아래처럼 넘파이 배열로\n","적재된 데이터 셋을 불러올땐 바로 사용해도 된다.\n","또 오히려 데이터셋을 읽어들이는 속도보다 tpu가 처리하는 속도가 빨라서\n","병목이 생길수도 있는데, 데이터 크기가 작다면 VM메모리에 데이터셋을 적재하면 되고\n","그렇지 않다면 데이터를 TFRecord 파일로 저장해 사용해야 한다.\"\"\"\n","\n","(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n","model.fit(x_train, y_train, batch_size = 1024)\n","# 배치 사이즈가 코어 개수만큼(코랩 tpu는 8코어) 쪼개지므로 크게 지정해야함.\n","\n","\"\"\"스텝 융합을 활용한 TPU 활용도 높이기:\n","작은 모델의 경우 배치 크기가 너무 커질 수 있는데, 이땐 옵티마이저의 학습률을 높일 수 있으며\n","혹은 compile 메서드에 steps_per_execution = 8 처럼 지정해서\n","TPU를 제대로 활용하지 못하는 모델의 속도를 극적으로 향상시킬 수 있음"],"metadata":{"id":"aWvnkHO7w66X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tip 3 케라스 튜너"],"metadata":{"id":"HhreXx-xwXJB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nNUWyDKudxEQ"},"outputs":[],"source":["!pip install keras-tuner -q"]},{"cell_type":"code","source":["# KerasTuner 모델 구축 함수\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","def build_model(hp):\n","    #유닛은 뉴런 개수를 의미한다 16 32 48 64개로 시도해보라는 뜻\n","    units = hp.Int(name = 'units', min_value = 16, max_value = 64, step = 16)\n","    model = keras.Sequential([\n","        layers.Dense(units, activation = 'relu'),\n","        layers.Dense(10, activation = 'softmax')\n","    ])\n","    optimizer = hp.Choice(name = 'optimizer', values = ['rmsprop','adam'])\n","    model.compile(\n","        optimizer = optimizer,\n","        loss = 'sparse_categorical_crossentropy',\n","        metrics = ['accuracy'])\n","    return model"],"metadata":{"id":"u_STqAijd6ne"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 또는 HyperModel 클래스의 객체 형태로 모델을 만들수도 있다.\n","\n","import keras_tuner as kt\n","\n","class SimpleMLP(kt.HyperModel):\n","    def __init__(self, num_classes):\n","        self.num_classes = num_classes\n","\n","    def build(self, hp): # 위 build_model함수와 동일\n","        units = hp.Int(name = 'units', min_value = 16, max_value = 64, step = 16)\n","        model = keras.Sequential([\n","            layers.Dense(units, activation = 'relu'),\n","            layers.Dense(10, activation = 'softmax')\n","        ])\n","        optimizer = hp.Choice(name = 'optimizer', values = ['rmsprop','adam'])\n","        model.compile(\n","            optimizer = optimizer,\n","            loss = 'sparse_categorical_crossentropy',\n","            metrics = ['accuracy'])\n","        return model\n","\n","hypermodel = SimpleMLP(num_classes = 10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IuD1Fvv_ekPB","executionInfo":{"status":"ok","timestamp":1698988480806,"user_tz":-540,"elapsed":996,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"3f381926-37bc-43ac-a6b4-3d67d6177b77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using TensorFlow backend\n"]}]},{"cell_type":"code","source":["tuner = kt.BayesianOptimization(\n","    build_model,  # 모델 구축 함수 지정\n","    # 최적화할 지표, 탐색 과정의 목표가 일반화할 모델을 찾는 것 이므로 항상 검증 지표를 지정해야한다.\n","    # 사용자 정의 지표라면 최소화할지 최대화할지 direction(max or min)을 설정해줘야함.\n","    objective = 'val_accuracy',\n","    max_trials = 100, # 탐색을 끝내기 까지 시도할 모델 설정의 최대 횟수\n","    executions_per_trial = 2, # 분산을 최소화 하기 위해 한 trial당 시도하고 평균낼 수 있음\n","    directory = 'mnist_kt_test', # 탐색 로그를 저장할 위치\n","    overwrite = True, #디렉토리 데이터를 덮어쓸지 여부, 모델 구축 함수를 수정했다면 True, 아니라면 False\n",")\n","\n","tuner.search_space_summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fgNIBLgCfHrM","executionInfo":{"status":"ok","timestamp":1698988486106,"user_tz":-540,"elapsed":3543,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"9d479b7d-c645-4c10-be12-eed9d4dd7edc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Search space summary\n","Default search space size: 2\n","units (Int)\n","{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 64, 'step': 16, 'sampling': 'linear'}\n","optimizer (Choice)\n","{'default': 'rmsprop', 'conditions': [], 'values': ['rmsprop', 'adam'], 'ordered': False}\n"]}]},{"cell_type":"code","source":["(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n","x_train = x_train.reshape((-1, 28 * 28)).astype(\"float32\") / 255\n","x_test = x_test.reshape((-1, 28 * 28)).astype(\"float32\") / 255\n","x_train_full = x_train[:]\n","y_train_full = y_train[:]\n","num_val_samples = 10000\n","x_train, x_val = x_train[:-num_val_samples], x_train[-num_val_samples:]\n","y_train, y_val = y_train[:-num_val_samples], y_train[-num_val_samples:]\n","callbacks = [\n","    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5),\n","]\n","tuner.search(\n","    x_train, y_train,\n","    batch_size=128,\n","    epochs=100,\n","    validation_data=(x_val, y_val),\n","    callbacks=callbacks,\n","    verbose=2,\n",")"],"metadata":{"id":"ToAYKHp2hWxn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["top_n = 4\n","best_hps = tuner.get_best_hyperparameters(top_n)"],"metadata":{"id":"uBQA2qP_j5sa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_best_epoch(hp):\n","    model = build_model(hp)\n","    callbacks=[\n","        keras.callbacks.EarlyStopping(\n","            monitor=\"val_loss\", mode=\"min\", patience=10)\n","    ]\n","    history = model.fit(\n","        x_train, y_train,\n","        validation_data=(x_val, y_val),\n","        epochs=100,\n","        batch_size=128,\n","        callbacks=callbacks)\n","    val_loss_per_epoch = history.history[\"val_loss\"]\n","    best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\n","    print(f\"Best epoch: {best_epoch}\")\n","    return best_epoch\n","\n","def get_best_trained_model(hp):\n","    best_epoch = get_best_epoch(hp)\n","    model.fit(\n","        x_train_full, y_train_full,\n","        batch_size=128, epochs=int(best_epoch * 1.2))\n","    return model\n","\n","best_models = []\n","for hp in best_hps:\n","    model = get_best_trained_model(hp)\n","    model.evaluate(x_test, y_test)\n","    best_models.append(model)"],"metadata":{"id":"B1mRJTplj61n"},"execution_count":null,"outputs":[]}]}