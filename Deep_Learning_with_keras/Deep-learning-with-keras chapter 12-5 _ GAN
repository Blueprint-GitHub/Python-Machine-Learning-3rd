{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZNviqrvDs8Rl"},"outputs":[],"source":["\"\"\" 간단한 GAN모델인 심층 합성곱 GAN (DCGAN)을 구현한다.\n","GAN 모델을 훈련할때 알아두어야 하는 지침\n","1. 특성 맵을 다운샘플링할때 풀링보다 스트라이드를 활용한다.\n","2. 균등 분포가 아니고 정규 분포를 활용해 잠재 공간에서 포인트를 샘플링한다.\n","3. 무작위성(판별자 레이블에 랜덤 레이블 추가)을 부여해 모델을 견고하게 만든다.\n","4. 그레이디언트가 희소되는것을 최소화 하기 위해 LeakyRelu 활성화 함수를 사용한다.\n","5. 커널 크기는 스트라이드 크기로 나누어질 수 있는 크기로 설정한다.(픽셀이 제대로 나오게) \"\"\"\n","\n","# 로컬에서 실행하는 경우 pip install gdown 명령으로 gdown 패키지를 먼저 설치.\n","!mkdir celeba_gan\n","!wget \"https://drive.google.com/uc?id=1up5bN8LCE2vHigVY-Z9yY2_aKRW5jN_9&confirm=t\" -O celeba_gan/data.zip\n","!unzip -qq celeba_gan/data.zip -d celeba_gan"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7317,"status":"ok","timestamp":1698983793560,"user":{"displayName":"김태식","userId":"11264777984025639378"},"user_tz":-540},"id":"VuRD3xXwUaKG","outputId":"846f8d8b-05a3-4269-d2d8-c1cdb9b7c442"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 202599 files belonging to 1 classes.\n"]}],"source":["# 데이터셋 생성\n","\n","from tensorflow import keras\n","\n","dataset = keras.utils.image_dataset_from_directory(\n","    \"celeba_gan\",\n","    label_mode = None,\n","    image_size = (64, 64),\n","    batch_size = 32,\n","    smart_resize = True) # 가로 세로 비율 유지\n","\n","dataset = dataset.map(lambda x: x / 255.0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KOYxdRnLVDUh"},"outputs":[],"source":["# GAN 판별자 네트워크\n","\n","from tensorflow.keras import layers\n","\n","discriminator = keras.Sequential(\n","    [\n","        keras.Input(shape = (64, 64, 3)),\n","        layers.Conv2D(64, kernel_size = 4, strides = 2, padding = 'same'),\n","        # 각 층 마다 BatchNormalization을 추가할 수 있다.(모멘텀 0.9) 다만 모드의 붕괴 주의\n","        layers.LeakyReLU(alpha = 0.2),\n","        layers.Conv2D(128, kernel_size = 4, strides = 2, padding = 'same'),\n","        layers.LeakyReLU(alpha = 0.2),\n","        layers.Conv2D(128, kernel_size = 4, strides = 2, padding = 'same'),\n","        layers.LeakyReLU(alpha = 0.2),\n","        layers.Flatten(),\n","        layers.Dropout(0.2), # dropout층 중요!\n","        layers.Dense(1, activation= 'sigmoid')\n","    ],\n","    name = 'discriminator')\n","\n","discriminator.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1eDDgXD8VwFm"},"outputs":[],"source":["# GAN 생성자 네트워크\n","latent_dim = 128\n","\n","generator = keras.Sequential(\n","    [\n","        keras.Input(shape = (latent_dim,)),\n","        layers.Dense(8 * 8 * 128), # 판별자의 Flatten층 크기\n","        layers.Reshape((8, 8, 128)), # Flatten 작업 복구\n","        layers.Conv2DTranspose(128, kernel_size = 4, strides = 2, padding = 'same'),\n","        layers.LeakyReLU(alpha = 0.2),\n","        layers.Conv2DTranspose(256, kernel_size = 4, strides = 2, padding = 'same'),\n","        layers.LeakyReLU(alpha = 0.2),\n","        layers.Conv2DTranspose(512, kernel_size = 4, strides = 2, padding = 'same'),\n","        layers.LeakyReLU(alpha = 0.2),\n","        layers.Conv2D(3, kernel_size = 5, padding = 'same', activation = 'sigmoid') # 출력크기 (28,28,1)\n","    ], name = 'generator')\n","\n","# 생성자는 출력을 sigmoid말고 tanh로 해야한다는 의견도 있음\n","# 학습을 하면서 그레이디언트가 0에 수렴해 생성자가 죽어버릴 수 있기 때문"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E7vhuMG5X5po"},"outputs":[],"source":["import tensorflow as tf\n","class GAN(keras.Model): #사용자 정의 모델\n","    def __init__(self, discriminator, generator, latent_dim):\n","        super().__init__()\n","        self.discriminator = discriminator\n","        self.generator = generator\n","        self.latent_dim = latent_dim\n","        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n","        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n","\n","    def compile(self, d_optimizer, g_optimizer, loss_fn):\n","        super(GAN, self).compile()\n","        self.d_optimizer = d_optimizer\n","        self.g_optimizer = g_optimizer\n","        self.loss_fn = loss_fn\n","\n","    @property\n","    def metrics(self):\n","        return [self.d_loss_metric, self.g_loss_metric]\n","\n","    def train_step(self, real_images): # 배치를 미니 배치로 쪼갤 수도 있음.\n","        batch_size = tf.shape(real_images)[0]\n","        #잠재 공간에서 랜덤 포인트 샘플링\n","        random_latent_vectors = tf.random.normal(\n","            shape=(batch_size, self.latent_dim))\n","        #랜덤 포인트를 가짜 이미지로 디코딩하고 진짜 이미지와 합침\n","        generated_images = self.generator(random_latent_vectors)\n","        combined_images = tf.concat([generated_images, real_images], axis=0)\n","        labels = tf.concat(\n","            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))],\n","            axis=0\n","        )\n","\n","        #레이블에 랜덤 잡음 추가\n","        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n","\n","        with tf.GradientTape() as tape: #판별자 훈련\n","            predictions = self.discriminator(combined_images)\n","            d_loss = self.loss_fn(labels, predictions)\n","        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n","        self.d_optimizer.apply_gradients(\n","            zip(grads, self.discriminator.trainable_weights)\n","        )\n","\n","        #잠재 공간에서 랜덤 포인트 샘플링\n","        random_latent_vectors = tf.random.normal(\n","            shape=(batch_size, self.latent_dim))\n","\n","        #모두 진짜 이미지라고 말하는 레이블 추가(사실 아니지만)\n","        misleading_labels = tf.zeros((batch_size, 1))\n","\n","        with tf.GradientTape() as tape: #생성자 훈련\n","            predictions = self.discriminator(\n","                self.generator(random_latent_vectors))\n","            g_loss = self.loss_fn(misleading_labels, predictions)\n","        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n","        self.g_optimizer.apply_gradients(\n","            zip(grads, self.generator.trainable_weights))\n","\n","        self.d_loss_metric.update_state(d_loss)\n","        self.g_loss_metric.update_state(g_loss)\n","        return {\"d_loss\": self.d_loss_metric.result(),\n","                \"g_loss\": self.g_loss_metric.result()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X7T4YHFpX7EP"},"outputs":[],"source":["class GANMonitor(keras.callbacks.Callback): # 가짜 이미지를 확인하기 위한 콜백\n","    def __init__(self, num_img=3, latent_dim=128):\n","        self.num_img = num_img\n","        self.latent_dim = latent_dim\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n","        generated_images = self.model.generator(random_latent_vectors)\n","        generated_images *= 255\n","        generated_images.numpy()\n","        for i in range(self.num_img):\n","            img = keras.utils.array_to_img(generated_images[i])\n","            img.save(f\"generated_img_{epoch:03d}_{i}.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"2YSARi7cX8L2","outputId":"375fa1d9-dff0-44d6-9e52-a876e1402ae6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/60\n","   3/6332 [..............................] - ETA: 33:26:27 - d_loss: 0.6817 - g_loss: 0.8010"]}],"source":["# 적대적 손실이 크게 증가하며 판별자 손실이 0으로 향하고\n","# 판별자가 생성자를 압도하는 것 처럼 보이면 판별자의 학습률을 낮추고\n","# 판별자의 드롭아웃 비율을 높여보고 다시 시도해본다.\n","\n","epochs = 60\n","\n","gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n","gan.compile(\n","    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n","    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n","    loss_fn=keras.losses.BinaryCrossentropy(),\n",")\n","\n","gan.fit(\n","    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",")"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyOtM4waP4roy/zMi7b8Uwjv"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}