{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPm+NbLtfH9YkPSHsK1K7os",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Blueprint-GitHub/Study_Note/blob/main/Machine_Learning_Engineering_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Feature Engineering**"
      ],
      "metadata": {
        "id": "acqRtqNBqM7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. 텍스트에 대한 특성 공학**"
      ],
      "metadata": {
        "id": "CGGV3hPJsO_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "1. One-Hot Encoding # sklearn.preprocessing.OneHotEncode\n",
        "  - 원-핫 인코딩은 범주형 속성을 여러 이진 속성으로 변환하는 방법으로 범주형 특성의 서로 다른 범주 개수만큼\n",
        "    희소 벡터를 생성해 그 범주는 1로 나머지는 0으로 채워넣어 이진 벡터로 표현한다.\n",
        "    {빨강, 파랑, 초록} -> [1, 0, 0], [0, 1, 0], [0, 0, 1]\n",
        "\n",
        "2. Bag-of-words # sklearn.feature_extraction.text.CountVectorizer\n",
        "  - Bow는 원-핫 인코딩 기법을 일반화해서 텍스트 데이터에 적용한 것으로 텍스트의 단어들을 토큰이라는 조각으로 분할하여\n",
        "    각 토큰에 고유한 색인(번호)를 할당한다. 그러면 텍스트로 된 문장을 어휘 사전(vocabulary)길이의 희소 벡터로 변환할 수 있다.\n",
        "    희소 벡터의 표현을 그 단어가 있냐없냐의 이진값으로 나타낼 수도 있지만, 토큰 수(Count Encoding)나 토큰 빈도,\n",
        "    TF-IDF(빈도-역문서 빈도; 자주 등장하는 단어의 가중치를 줄여준다)와 같은 형식으로도 표현할 수 있다.\n",
        "\n",
        "3. bag-of-n-grams # sklearn.feature_extraction.text.CountVectorizer(ngram_range = n)\n",
        "  - Bow에 n-garm을 적용해 더욱 확장할 수 있는데 이렇게 하면 모델이 더욱 정교한 표현을 학습할 수 있게 되지만\n",
        "    더 희소한 특성 벡터가 만들어진다는 단점이 있다. n-gram은 여러 단어로 구성된 토큰으로 분할하는 방법으로\n",
        "    I am your father 라는 문장을 [I am, am your, your father]와 같이 2-gram(bigrams)으로 분할할 수 있다.\n",
        "\n",
        "4. Mean Encoding (Target Encoding) # category_encoders.target_encoder.TargetEncoder\n",
        "  - 범주형 특성에 대한 레이블값의 평균으로 범주형 특성을 대체하는 방법. 데이터 차원이 증가하지 않고 레이블에 대한\n",
        "    일부 정보가 포함된다는 장점이 있다. (훈련 데이터에서 계산한 평균값을 사용하지 않으면 데이터 누수가 발생할 수 있다.)\n",
        "    샘플 수가 적은 범주에서 과적합의 위험이 있으며 평균값 계산 시 정규화나 스무딩 기법을 사용 가능하다.\n",
        "\n",
        "5. Odds Ratio\n",
        "  - 승산비(Odds ratio)는 일반적으로 두 사건 A와 B 사이의 연관 강도를 정량화하는 통계로. 승산비가 1인 경우\n",
        "    두 사건은 독립적이라고 판단할 수 있다. 승산비는 매우 낮거나(0에 가깝게) 매우 크게 (100 이상) 나타날 수 있으며\n",
        "    이러한 수치적 오버플로를 방지하기 위해 종종 로그 승산비를 적용 할 수 있으며. 범주형 특성 이렇게 산출된 승산비\n",
        "    값으로 대체하여 사용할 수 있다.\n",
        "\n",
        "6. Ordinal Scaling\n",
        "  - 때로는 범주형 특징에 순서는 있지만 각 특징간 비율이 다를 수 있다. 예를 들면 학교 성적(A에서 E까지)나\n",
        "    연공서열 수준(하급, 중급, 상급)처럼 일부 값이 더 멀리 떨어져야 하는 경우 하급,중급,상급을 1/5, 2/5, 1 등으로\n",
        "    사용할 수 있다. 이러한 경우는 적절한 도메인 지식이 필요하며 각 범주의 실제 가치를 반영하는 적절한 스케일을 선택해야 한다.\n",
        "\n",
        "7. 사인-코사인 변환\n",
        "  - 범주형 특성에 주기성이 있는 경우에는 정수 인코딩이 제대로 작동하지 않는다. 예를 들어 요일을 OrdinalEncoder로 변환해\n",
        "    1~7로 나타내면 월요일(1)과 일요일(7)의 차이는 6이지만 각 요일의 차이는 1이어야 한다. 따라서 이 대신에\n",
        "    사인-코사인 변환을 적용해 각 요일 간 간격이 동일하게 만들 수 있다.\n",
        "\n",
        "8. Feature Hashing #MurmurHash3, Jenkins, CityHash, MD5\n",
        "  - 특성 해싱 혹은 해싱 트릭은 텍스트 데이터가 값이 많은 범주형 속성을 임의 차원의 특성 벡터로 변환한다.\n",
        "    만약 텍스트 문서 모음에 100만개의 고유한 토큰이 있다면 계산 비용이 많이 들 수 있는데, 특성 해싱을 적용하면\n",
        "    각 특성에 대해 해시 함수를 적용하여 고정된 크기의 벡터에서의 인덱스를 바로바로 계산할 수 있게 되므로\n",
        "    메모리를 효율적으로 사용할 수 있고 병렬 처리가 용이해지며 학습 변환 과정이 단순해지는 장점이 있다.\n",
        "    그러나 인덱스를 저장할 특성 벡터의 크기가 작을 경우 서로 다른 특성이 같은 인덱스에 할당되는 충돌이 발생할 수 있는데\n",
        "    해시 공간을 충분히 크게 하여 충돌을 최소화할 수 있지만 학습속도가 느려질 수 있다.\n",
        "\n",
        "9. 주제 모델링(LSA, LDA) # sklearn.decomposition.TruncatedSVD, LatentDirichletAllocation\n",
        "  - 일반적으로 주제 모델링은 자연어 텍스트 문서 형태의 레이블링되지 않은 데이터를 사용하는 기술로 문서를 주제를 나타내는\n",
        "    벡터로 표현하는 방법을 배운다. 주제 모델링에는 잠재 의미 분석(Latent Semantic Analysis, LSA)과\n",
        "    잠재 디리클레 할당(Latent Dirichlet Allocation,LDA)과 같은 알고리즘이 있는데 LSA는 Bow나 TF-IDF 벡터를 입력으로 받아\n",
        "    특잇값 분해(Signlar Value Decomposition, SVD)를 통해 중요한 구조를 포착하는 반면 LDA는 문서가 여러 개의 주제로\n",
        "    구성되어 있다고 가정하고 계층적 베이지안 모델을 기반으로 각 문서의 주제 분포와 주제 내의 단어 분포를 학습한다.\n",
        "\n",
        "10. NMF (Non-negative Matrix Factorization) # sklearn.decomposition.NMF\n",
        "  - NMF는 복잡한 데이터를 더 이해하기 쉬운 두 개의 행렬로 분해하여 데이터를 더 간단하게 파악할 수 있게 해주는 도구로\n",
        "    원본 행렬을 분해한다. 만약 영화A,B,C에 대한 고객들의 별점 데이터 셋이 있을때 NMF를 통해 영화의 장르(특성 행렬)와\n",
        "    고객의 장르 선호도(계수 행렬)로 나눌 수 있다."
      ],
      "metadata": {
        "id": "RegK2knlqP5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. 시계열 특징**"
      ],
      "metadata": {
        "id": "dg8vAtgkChY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "시계열 데이터(time-series data)는 전통적인 지도학습 데이터와 달리 타임스탬프, 날짜, 월-연도와 같은 시간 속성을 포함한다.\n",
        "기존 시계열 데이터(classical time-series data)는 분당 1회, 시간당 1회 등과 같이 시간에 따라 균등한 간격으로 관측되지만\n",
        "한 시간에 다섯 번, 하루에 세 번 등과 같이 비정기적(불규칙)으로 관측되는 데이터도 있을 수 있는데, 이를 점 프로세스(dot process)\n",
        "또는 이벤트 스트림(event stream)이라고 한다."
      ],
      "metadata": {
        "id": "zQUV8iOICx_j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}