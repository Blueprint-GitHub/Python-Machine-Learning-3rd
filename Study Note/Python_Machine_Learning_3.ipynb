{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjWlU1uOBfMmcBkbOPRTmb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Blueprint-GitHub/Python-Machine-Learning-3rd/blob/main/Python_Machine_Learning_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **결정 경계 그래프 함수**"
      ],
      "metadata": {
        "id": "wECD1tKWVcnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "\n",
        "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
        "\n",
        "    # 마커와 컬러맵을 설정합니다\n",
        "    markers = ('s', 'x', 'o', '^', 'v')\n",
        "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "    # 결정 경계를 그립니다\n",
        "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "                           np.arange(x2_min, x2_max, resolution))\n",
        "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "    Z = Z.reshape(xx1.shape)\n",
        "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
        "    plt.xlim(xx1.min(), xx1.max())\n",
        "    plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "    # 샘플의 산점도를 그립니다\n",
        "    for idx, cl in enumerate(np.unique(y)):\n",
        "        plt.scatter(x=X[y == cl, 0],\n",
        "                    y=X[y == cl, 1],\n",
        "                    alpha=0.8,\n",
        "                    c=colors[idx],\n",
        "                    marker=markers[idx],\n",
        "                    label=cl,\n",
        "                    edgecolor=None if idx==1 else 'black')"
      ],
      "metadata": {
        "id": "DqQoM5oiVWC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3장 머신러닝 분류 모델**"
      ],
      "metadata": {
        "id": "xU_xRyOITKfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "121 / SVM 모델에 규제를 가할때는 gamma와 C 매개변수를 동시에 조절하는 것이 좋다."
      ],
      "metadata": {
        "id": "H2gQM45jVRpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4장 누락된 데이터 다루기**"
      ],
      "metadata": {
        "id": "lipWQ_D6TKrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "148 / SimpleImputer 말고 FunctionTransformer 함수를 사용하면 사용자 정의 함수를 구현하여\n",
        "      데이터를 조작할 수 있는데 예를 들면 짝수 열은 평균, 홀수 열은 최빈값으로 보간한다던지,\n",
        "      해당 열에 로그를 적용한다던지, 특정값보다 큰 값을 다른 값으로 대체한다던지 등의 일을 수행할 수 있다.\n",
        "      def change:; X[X>5] = -1; return X 으로 5보다 큰 값은 -1로 변환하는 함수를 구현하고\n",
        "      FunctionTransformer(change)로 쓴다.\n",
        "\n",
        "151 / df.fillna(method = 'backfill' , 'ffill' , 'ffill' axis = 1) 로 지정해서 누락된 값을\n",
        "      다음 행의 값, 이전 행의 값, 이전 열의 값으로 채울 수 있다.\n",
        "\n",
        "154 / 순서가 있는 범주형 특성 매핑은 {'XL' : 3 , 'L' : 2, 'M' : 1}등의 딕셔너리를\n",
        "      구현하고 df['size'] = df['size'].map(dict)로 매핑하여 정수로 변환할 수 있다.\n",
        "\n",
        "159 / 원-핫 인코딩을 편하게 만드는 방법은 pd.get_dummies(df['color'])등으로 구현하는 것인데\n",
        "      이때 2개 이상의 열이 생기면 그 중 하나를 지워도 정보를 잃지 않으므로\n",
        "      pd.get_dummies(df['color'], drop = True)로 지정해 다중공선성 문제를 조금 줄일 수 있다.\n",
        "      또 pd.get_dummies(df[['size', 'color']], columns = 'size')로 원핫인코딩을 수행할 열을 지정할 수 있다.\n",
        "\n",
        "167 / 이상치가 많이 포함되거나 작은 데이터셋은 과대작합되기 쉬운데 이때는 전처리할때 StandardScaler 대신\n",
        "      RobustScaler를 사용할 수 있다. 이 함수는 극단적인 값과 이상치에 영향을 덜 받는다.\n",
        "\n",
        "172,173 / L2 규제는 최소 비용에 다가가진 못하더라도 패널티를 섞어 더욱 일반화되도록 만들고\n",
        "          L1 규제는 특정 특성값이 0 일때 최소 비용에 가장 가까운 형태를 보여 희소성을 나타나게 한다.\n",
        "\n",
        "182 / SequentialFeatureSelector(estimator, n_features_to_select = n)를 사용해 순차 특성 선택을 구현할 수 있다.\n",
        "      반복문을 구현해 1개의 특성부터 전체 특성까지 선택하고 모델을 훈련해보면서 정확도 그래프를 그릴 수 있다.\n",
        "\n",
        "187 / tree모델은 특성 중요도를 반환하는데 RFE(forest, n_features_to_select = n)을 설정해 특성 중요도에 따라\n",
        "      특성을 선택할 수 있다. 전체 특성부터 시작해 n개의 특성에 도달할 때 까지 특성 중요도가 가장 낮은 특성을\n",
        "      하나씩 지우면서 진행된다."
      ],
      "metadata": {
        "id": "TolEBQVNVCWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5장 차원 축소**"
      ],
      "metadata": {
        "id": "rCtMQSPrTK2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "193 / PCA를 사용하기 전에는 데이터를 전처리 해야한다.\n",
        "\n",
        "203 / 각 차원의 설명된 분산을 알고 싶으면 PCA(n_components = None)으로 지정하고 pca.fit_transform(X) 한 다음\n",
        "      pca.explained_variance_ratio_ 를 입력하면 된다.\n",
        "\n",
        "204 / PCA(n_components = 'mle')로 지정하면 토마스 민카 알고리즘에 따라 자동으로 적절한 차원으로 변환해준다.\n",
        "       이는 특성이 너무 많고 파악하기가 어려울때 사용해보기 좋다.\n",
        "\n",
        "206 / LDA란 선형 판별 분석으로, 각 클래스 간 구분을 잘 해주고, 클래스 내 분산은 최대화해주는 방향으로\n",
        "      선형 판별 벡터를 선정해 차원을 축소시켜주는 방법이다. pca와 달리 lda.fit_transform(X, y) 처럼\n",
        "      레이블을 지정해주어야 한다. (지도 학습이다.)\n",
        "\n",
        "224 / RBF 커널PCA는 감마 값을 튜닝해주어야 하는데 그리드 서치로 찾는것이 효율적이다.\n"
      ],
      "metadata": {
        "id": "zfwSaikyU-jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6장 모델 평가와 하이퍼파라미터 튜닝**"
      ],
      "metadata": {
        "id": "qWVgkL2sTLg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "252 / cross_val_score는 측정 지표를 지정할 수 없는 반면 cross_validate는 (여러 개의)측정 지표를 지정할 수 있다.\n",
        "\n",
        "257 / train_size, train_score, test_score = learning_curve(estimators, X, y);\n",
        "      LearningCurveDisplay(train_size, train_score, test_score, score_name = 'score')\n",
        "      train_score, test_score = validation_curve(estimators, X, y);\n",
        "      ValidationCurveDisplay(train_score, test_socre, score_name = 'score') 을 사용해서\n",
        "      학습 곡선과 검증 곡선을 그릴 수 있다. 이는 모델의 분산,편향을 파악하거나 혹은\n",
        "      특정 하이퍼파라미터의 변화에 따른 정확도 곡선을 그릴 수 있게 해준다.\n",
        "\n",
        "262 / grid Search로 찾는 best_estimator_는 자동으로 훈련 데이터로 학습되므로(refit = True) 다시 학습할 필요 없이\n",
        "      바로 test set에 사용할 수 있다.\n",
        "\n",
        "265 / keras Tuner의 hyperband 방식과 유사한 HalvingGridSearch는 사용하려면\n",
        "      from sklearn.experimental import enable_halving_search_cv 를 불러와야 한다.\n",
        "\n",
        "268 / 중첩 교차 검증 : 그리드 서치 모델을 cross_val_score 등의 교차검증에 estimator로 넣어서\n",
        "      중첩 교차를 사용할 수 있다.\n",
        "\n",
        "284 / from imblearn.over_sampling import SMOTE ; over = SMOTE(sampling_strategy=0.1) ;\n",
        "      X_sample, y_sample = fit_resample(X, y) 과 같은 방식으로 인공적인 샘플을 생성할 수 있다.\n",
        "      sklearn의 resample 함수는 단순복제이다."
      ],
      "metadata": {
        "id": "-7RTvs3-U6P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7장 앙상블**"
      ],
      "metadata": {
        "id": "fhqGqN-rTP9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "306 / 앙상블 모델의 .get_params()를 통해 그 안에 모든 모델의 하이퍼파라미터 목록을 볼 수 있다.\n",
        "\n",
        "311 / 배깅 알고리즘은 일반적으로 가지치기하지 않는 결정 트리를 분류기로 사용한다.(max_depth = None)\n",
        "      왜냐하면 배깅은 모델의 분산을 감소시키는 효과가 있지만 편향을 감소시키지는 않기 때문에,\n",
        "      편향이 낮은 모델(가지치기하지 않는 결정 트리)를 사용하는 것이다.\n",
        "\n",
        "327 / 그레이디언트 부스팅 모델의 subsample 매개변수를 1 이하로 지정하면 그 비율만큼 랜덤하게 샘플링해\n",
        "      트리를 훈련하는데, 이것은 부트스트랩과 유사하게 동작하며,\n",
        "      oob_loss = np.cumsum(-gbrt.oob_improvement_)\n",
        "      plt.plot(range(100), oob_loss) 과 같은 코드로 어느 시점의 n_estimators에서 과대적합이 시작되는지 볼 수 있다.\n",
        "      굳이 그래프를 그리지 않고도 n_iter_no_change 매개변수로 조기종료를 지정해 사용할수도 있다."
      ],
      "metadata": {
        "id": "QHIOwlNOU2Eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8장 감성 분석에 머신 러닝 응용**"
      ],
      "metadata": {
        "id": "Qe8etCi4Tkdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "344 / 텍스트 분류에서 어간 추출과 표제어 추출은 성능에 영향을 크게 미치지 않는다.\n",
        "\n",
        "347 / 한국어는 어간 추출이나 Bag of Word 방식보다 표제어 추출 방식이 적합하며 이를 형태소 분석이라 한다.\n",
        "      한글 형태소 분석을 위한 대표적인 패키지는 konlpy 와 soynlp가 있으며 이 중 konlpy가 더 사용하기 좋아보인다.\n",
        "\n",
        "348 / read_csv로 파일을 읽어들일때 기본적으로 콤마를 기준으로 필드를 구분하지만 그렇지 않을때\n",
        "      delimiter = '\\t' 로(탭) 지정하여 필드 구분자를 변경할 수 있다."
      ],
      "metadata": {
        "id": "Y33qMI3bUula"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10장 회귀 분석으로 연속적 타깃 변수 예측**"
      ],
      "metadata": {
        "id": "75opKSjwUxVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "432 / 어떤 특성에 이차, 삼차 다항식을 추가해 타겟값과 더욱 유사한 관계를 가지도록 할 수 있지만\n",
        "      때로는 특성을 로그 스케일로 변환하고 타겟값에 제곱근을 취해 선형 관계로 변환하는 것이 더 효과적일때도 있다."
      ],
      "metadata": {
        "id": "ETwhKHmdUyix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **11장 레이블되지 않은 데이터 다루기 : 군집 분석**"
      ],
      "metadata": {
        "id": "RM2tOshATkuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "449 / k-mean을 hard cluster(직접 군집)이라고 하면 샘플이 각 클러스터에 소속될 확률을 반환하는 것을\n",
        "      soft cluster(간접 군집)이라고 하는데, 간접 군집의 대표적인 알고리즘은 FCM이 있다. FCM은 sklearn에는\n",
        "      구현되어 있지 않으며, k-mean과 성능 차이가 별로 없다.\n",
        "\n",
        "457 / 계층 군집이란 가장 가까운 샘플들을 합쳐 단일 클러스터를 만드는 방법으로 그 과정을 그래프로 그릴 수 있다.\n",
        "      이 그래프를 덴드로그램이라고 하며, 트리 구조로 나타낸다. 계층 군집의 장점은 k-mean처럼 엘보우나 실루엣 점수를\n",
        "      활용해 적절한 k값을 찾을 필요가 없다는 점이며, 최종 클러스터의 수를 정할수도 있다.\n",
        "      실전에서는 덴드로그램과 히트맵을 함께 표시해서 데이터셋을 쉽게 요약하는 방법으로 주로 사용한다.\n",
        "      sklearn의 AgglomerativeClustering을 사용하면 병합 군집을 적용할 수 있지만 시각화를 위해서는 사이파이가 유리하다.\n",
        "\n",
        "471 / 임의 형태의 데이터(반달 데이터)는 DBSCAN(밀집기반)을 사용하면 효과적으로 사용할 수 있으나, 다른 군집 알고리즘과\n",
        "      마찬가지로 특성 개수가 늘어나면 차원의 저주가 생길 수 있으며, 이로 인해 일반적으로 군집을 사용하기 전에는\n",
        "      PCA나 RBF커널 주성분 분석 같은 차원 축소 기법이 선행된다. 또 데이터셋을 2차원으로 축소하면 결과를\n",
        "      시각화하기에 매우 유리하다."
      ],
      "metadata": {
        "id": "7aAXgRv3UpR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **12장 다층 인공 신경망을 밑바닥부터 구현**"
      ],
      "metadata": {
        "id": "ba9P3-QiTrLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "489 / 넘파이 배열을 사용할 때 다차원 배열을 디스크에 저장하는 효율적이고\n",
        "      가장 간편한 방법은 np.savez함수를 사용하는 것이다.\n",
        "      파이썬의 pickle 모듈과 비슷하게 생각하면 된다. 데이터를 여러 번 불러오고\n",
        "      전처리를 매번 하는것은 비효율적이므로 전처리를 한 데이터를 저장하고\n",
        "      그것을 불러오면서 사용하는 것이 권장된다."
      ],
      "metadata": {
        "id": "4uIDwHNRUigR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **13장 텐서플로를 사용한 신경망 훈련**"
      ],
      "metadata": {
        "id": "AY7ucWoOTx3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 자주 사용하는 텐서플로우 함수 ###\n",
        "    tf.convert_to_tensor(a) : 넘파이 배열 a 텐서로 변환\n",
        "    tf.cast : 데이터 타입 변경,\n",
        "    tf.transpose : 텐서 전치,\n",
        "    tf.reshape : 텐서 크기 변경,\n",
        "    tf.squeeze : 불필요한 차원 삭제(크기가 1인 차원)\n",
        "    tf.multiply(t1, t2) : 텐서 곱셈\n",
        "    tf.math.reduce_mean / tf.math.reduce_sum / tf.math.reduce_std : 평균, 합, 표준 편차\n",
        "    tf.linalg.matmul(t1, t2) : 행렬 곱셈\n",
        "    tf.norm : 텐서 노름 계산\n",
        "    tf.split(t1, n) : 텐서를 n개로 분할 ([3,2])처럼 전달해 각각 분할할 크기를 정할수도 있다.\n",
        "  * tf.concat, tf.stack : 두 텐서 일렬로 연결 및 쌓기(차원 증가)\n",
        "    tf.data.Dataset.from_tensor_slices(t1) : 텐서플로 데이터셋 생성\n",
        "  * tf.data.Dataset.from_tensor_slices(t1, t2) : 두 데이터셋을 연결한 텐서(feature와 label을 묶을때 사용)\n",
        "    dataset.shuffle(buffer_size = None) : 데이터셋 섞기 이때 buffer_size를 작게 하면 데이터셋을 덜 섞게 되고, 샘플 크기와\n",
        "                                        동일하게 설정하면 에포크마다 샘플이 완전하게 섞인다.\n",
        "\n",
        "### tf.io ###\n",
        "    '''파일 입출력과 데이터 파싱을 위한 기능을 제공.'''\n",
        "  * tf.io.read_file(filename): 파일을 읽어 문자열로 반환.\n",
        "    tf.io.write_file(filename, contents): 데이터 저장.\n",
        "    tf.io.decode_csv(records, record_defaults): CSV 파일을 읽어 TensorFlow에서 사용할 수 있는 형태로 디코딩.\n",
        "    tf.io.decode_image(contents, channels=0): 이미지 파일을 읽어 텐서로 디코딩.\n",
        "    tf.io.decode_json_example(json_examples): JSON 형식의 문자열을 파싱하여 텐서로 변환.\n",
        "    tf.io.parse_example(serialized, features): tf.train.Example로 직렬화된 데이터를 파싱.\n",
        "    tf.io.TFRecordWriter(path): 데이터를 TFRecord 파일로 쓰기 위한 라이터를 생성.\n",
        "\n",
        "### tf.image ###\n",
        "    '''이미지 데이터를 처리하기 위한 다양한 함수를 제공.'''\n",
        "  * tf.image.decode_jpeg(contents): JPEG 형식의 이미지를 텐서로 디코딩.\n",
        "    tf.image.resize(images, size): 이미지 크기를 조정.\n",
        "    tf.image.flip_left_right(image): 이미지 좌우로 뒤집기.\n",
        "    tf.image.adjust_brightness(image, delta): 이미지 밝기 조절.\n",
        "    tf.image.adjust_contrast(image, contrast_factor): 이미지 대비 조절.\n",
        "    tf.image.rgb_to_grayscale(images): RGB 이미지를 그레이스케일로 변환.\n",
        "    tf.image.rot90(image, k=1): 이미지를 90도씩 회전.\n",
        "\n",
        "    '''이미지 파일을 텐서로 읽어들일때 다음과 같이 사용한다.'''\n",
        "    filename = 'path/to/your/image.jpg'\n",
        "    # 파일 읽기\n",
        "    image_string = tf.io.read_file(filename)\n",
        "    # 이미지 디코딩\n",
        "    image = tf.image.decode_jpeg(image_string)"
      ],
      "metadata": {
        "id": "4M8uCGH3Xb55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mke1rWH-FzCZ"
      },
      "outputs": [],
      "source": [
        "523 / 텐서를 만들때 tf.fill((2,3) ,1) 로 2*3 행렬을 1로 가득 채울 수 있다.\n",
        "      tf.one_hot([1,2,3],4)와 같이 써서 원핫인코딩 행렬을\n",
        "      만들수도 있는데. [1,2,3]이 1이 오는 인덱스고 리스트의 길이가 행이며, 4가 열이다.\n",
        "\n",
        "542 / tensorflow dataset에서 훈련을 위한 데이터를 불러올때는 다음과 같이 한다\n",
        "      1. data, data_info = tfds.load('mnist', with_info = True, shuffle_files = False)\n",
        "      2. print(data.keys()) #데이터셋 확인\n",
        "      3. train_data = data['train']\n",
        "      4. train_data = train_data.map(lambda item: (item['image'], item['label']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **15장 심층 합성곱 신경망으로 이미지 분류**"
      ],
      "metadata": {
        "id": "NSQffJq5mqAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "621 / CNN은 유용한 특성을 학습하는 기능이 있으므로 이미지 문제가 아니더라도\n",
        "      featuretools + CNN + MLP 을 사용해서 더욱 성능을 끌어올릴 수 있다.\n",
        "\n",
        "633 / 겹침 풀링(Overlapping Pooling)은 CNN의 풀링 레이어에서 stride를 window\n",
        "      크기보다 작게 해 Feature map의 중요한 정보를 더 잘 보존하도록 하는 기술이다.\n",
        "      이로인해 특징 추출의 강건성(Robust)이 증가하지만 계산 복잡도가 증가하고\n",
        "      과적합이 발생할 위험이 있다.\n",
        "\n",
        "641 / Keras의 CrossEntropy 손실 함수에서 from_logit = True 로 사용하는것이 권장된다.\n",
        "      이 경우 출력층에 sigmoid나 softmax같은 활성화 함수를 쓰지 않아야 하며\n",
        "      활성화 함수를 거치지 않고 손실을 계산할 수 있기 때문에 더욱 정확하고 안정적이다.\n",
        "\n",
        "662 / model.fit(epochs = 30, initial_epoch = 20)으로 지정하면, 20번의 에포크를\n",
        "      이전에 훈련했을때 거기서부터 이어서 10번의 에포크를 더 진행할 수 있다."
      ],
      "metadata": {
        "id": "zqYsw1kBmsW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **16장 순환 신경망으로 순차 데이터 모델링**"
      ],
      "metadata": {
        "id": "wGVaLzrts8sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "688 / RNN을 훈련할때 사용되는 데이터를 배치로 다룰때는 각각의 미니 배치 안에\n",
        "      들어 있는 샘플들의 크기(시퀀스 길이)가 같아야 한다. 이를 위해서\n",
        "      dataset.padded_batch(batch_size, padded_shapes = ([-1], []))를 사용한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "XKJy8yqxs_cy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
