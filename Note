3장 머신러닝 분류 모델
121 / SVM 모델에 규제를 가할때는 gamma와 C 매개변수를 동시에 조절하는 것이 좋다.

4장 누락된 데이터 다루기
148 / SimpleImputer 말고 FunctionTransformer 함수를 사용하면 사용자 정의 함수를 구현하여 데이터를 조작할 수 있는데
       예를 들면 짝수 열은 평균, 홀수 열은 최빈값으로 보간한다던지, 해당 열에 로그를 적용한다던지, 특정값보다 큰 값을
       다른 값으로 대체한다던지 등의 일을 수행할 수 있다.
       def change:; X[X>5] = -1; return X 으로 5보다 큰 값은 -1로 변환하는 함수를 구현하고 FunctionTransformer(change)로 쓴다.
151 / df.fillna(method = 'backfill' , 'ffill' , 'ffill' axis = 1) 로 지정해서 누락된 값을 다음 행의 값, 이전 행의 값, 이전 열의 값으로
       채울 수 있다.
154 / 순서가 있는 범주형 특성 매핑은 {'XL' : 3 , 'L' : 2, 'M' : 1}등의 딕셔너리를 구현하고 df['size'] = df['size'].map(dict)로
        매핑하여 정수로 변환할 수 있다.
159 / 원-핫 인코딩을 편하게 만드는 방법은 pd.get_dummies(df['color'])등으로 구현하는 것인데 이때 2개 이상의 열이 생기면
        그 중 하나를 지워도 정보를 잃지 않으므로 pd.get_dummies(df['color'], drop = True)로 지정해 다중공선성 문제를
        조금 줄일 수 있다.
        또 pd.get_dummies(df[['size', 'color']], columns = 'size')로 원핫인코딩을 수행할 열을 지정할 수 있다.
167 / 이상치가 많이 포함되거나 작은 데이터셋은 과대작합되기 쉬운데 이때는 전처리할때 StandardScaler 대신
        RobustScaler를 사용할 수 있다. 이 함수는 극단적인 값과 이상치에 영향을 덜 받는다.
172,173 / L2 규제는 최소 비용에 다가가진 못하더라도 패널티를 섞어 더욱 일반화되도록 만들고
             L1 규제는 특정 특성값이 0 일때 최소 비용에 가장 가까운 형태를 보여 희소성을 나타나게 한다.
182 / SequentialFeatureSelector(estimator, n_features_to_select = n)를 사용해 순차 특성 선택을 구현할 수 있다.
       반복문을 구현해 1개의 특성부터 전체 특성까지 선택하고 모델을 훈련해보면서 정확도 그래프를 그릴 수 있다.
187 / tree모델은 특성 중요도를 반환하는데 RFE(forest, n_features_to_select = n)을 설정해 특성 중요도에 따라
        특성을 선택할 수 있다. 전체 특성부터 시작해 n개의 특성에 도달할 때 까지 특성 중요도가 가장 낮은 특성을
        하나씩 지우면서 진행된다.
5장 차원 축소
193 / PCA를 사용하기 전에는 데이터를 전처리 해야한다.
201 / 최대 5개 클래스를 지원하는 결정 경계 함수를 github에 보관해놓자.
203 / 각 차원의 설명된 분산을 알고 싶으면 PCA(n_components = None)으로 지정하고 pca.fit_transform(X) 한 다음
        pca.explained_variance_ratio_ 를 입력하면 된다.
204 / PCA(n_components = 'mle')로 지정하면 토마스 민카 알고리즘에 따라 자동으로 적절한 차원으로 변환해준다.
       이는 특성이 너무 많고 파악하기가 어려울때 사용해보기 좋다.
206 / LDA란 선형 판별 분석으로, 각 클래스 간 구분을 잘 해주고, 클래스 내 분산은 최대화해주는 방향으로
        선형 판별 벡터를 선정해 차원을 축소시켜주는 방법이다. pca와 달리 lda.fit_transform(X, y) 처럼
        레이블을 지정해주어야 한다. (지도 학습이다.)
224 / RBF 커널PCA는 감마 값을 튜닝해주어야 하는데 그리드 서치로 찾는것이 효율적이다.
 

6장 모델 평가와 하이퍼파라미터 튜닝

252 / cross_val_score는 측정 지표를 지정할 수 없는 반면 cross_validate는 (여러 개의)측정 지표를 지정할 수 있다.
257 / train_size, train_score, test_score = learning_curve(estimators, X, y);
       LearningCurveDisplay(train_size, train_score, test_score, score_name = 'score')
        train_score, test_score = validation_curve(estimators, X, y);
        ValidationCurveDisplay(train_score, test_socre, score_name = 'score') 을 사용해서
        학습 곡선과 검증 곡선을 그릴 수 있다. 이는 모델의 분산,편향을 파악하거나 혹은
        특정 하이퍼파라미터의 변화에 따른 정확도 곡선을 그릴 수 있게 해준다.
262 / grid Search로 찾는 best_estimator_는 자동으로 훈련 데이터로 학습되므로(refit = True) 다시 학습할 필요 없이
       바로 test set에 사용할 수 있다.
265 / keras Tuner의 hyperband 방식과 유사한 HalvingGridSearch는 사용하려면
       from sklearn.experimental import enable_halving_search_cv 를 불러와야 한다.
268 / 중첩 교차 검증 : 그리드 서치 모델을 cross_val_score 등의 교차검증에 estimator로 넣어서
       중첩 교차를 사용할 수 있다.
284 /  from imblearn.over_sampling import SMOTE ; over = SMOTE(sampling_strategy=0.1) ;
        X_sample, y_sample = fit_resample(X, y) 과 같은 방식으로 인공적인 샘플을 생성할 수 있다.
        sklearn의 resample 함수는 단순복제이다.

7장 앙상블
306 / 앙상블 모델의 .get_params()를 통해 그 안에 모든 모델의 하이퍼파라미터 목록을 볼 수 있다.
311 / 배깅 알고리즘은 일반적으로 가지치기하지 않는 결정 트리를 분류기로 사용한다.(max_depth = None)
        왜냐하면 배깅은 모델의 분산을 감소시키는 효과가 있지만 편향을 감소시키지는 않기 때문에,
        편향이 낮은 모델(가지치기하지 않는 결정 트리)를 사용하는 것이다.
327 / 그레이디언트 부스팅 모델의 subsample 매개변수를 1 이하로 지정하면 그 비율만큼 랜덤하게 샘플링해
        트리를 훈련하는데, 이것은 부트스트랩과 유사하게 동작하며,
        oob_loss = np.cumsum(-gbrt.oob_improvement_)
        plt.plot(range(100), oob_loss) 과 같은 코드로 어느 시점의 n_estimators에서 과대적합이 시작되는지 볼 수 있다.
        굳이 그래프를 그리지 않고도 n_iter_no_change 매개변수로 조기종료를 지정해 사용할수도 있다.


8장 감성 분석에 머신 러닝 응용
344 / 텍스트 분류에서 어간 추출과 표제어 추출은 성능에 영향을 크게 미치지 않는다.
347 / 한국어는 어간 추출이나 Bag of Word 방식보다 표제어 추출 방식이 적합하며 이를 형태소 분석이라 한다.
        한글 형태소 분석을 위한 대표적인 패키지는 konlpy 와 soynlp가 있으며 이 중 konlpy가 더 사용하기 좋아보인다.
348 / read_csv로 파일을 읽어들일때 기본적으로 콤마를 기준으로 필드를 구분하지만 그렇지 않을때
       delimiter = '\t' 로(탭) 지정하여 필드 구분자를 변경할 수 있다.

10장 회귀 분석으로 연속적 타깃 변수 예측
432 / 어떤 특성에 이차, 삼차 다항식을 추가해 타겟값과 더욱 유사한 관계를 가지도록 할 수 있지만
        때로는 특성을 로그 스케일로 변환하고 타겟값에 제곱근을 취해 선형 관계로 변환하는 것이 더 효과적일때도 있다.

11장 레이블되지 않은 데이터 다루기 : 군집 분석
449 / k-mean을 hard cluster(직접 군집)이라고 하면 샘플이 각 클러스터에 소속될 확률을 반환하는 것을
       soft cluster(간접 군집)이라고 하는데, 간접 군집의 대표적인 알고리즘은 FCM이 있다. FCM은 sklearn에는
       구현되어 있지 않으며, k-mean과 성능 차이가 별로 없다.
457 / 계층 군집이란 가장 가까운 샘플들을 합쳐 단일 클러스터를 만드는 방법으로 그 과정을 그래프로 그릴 수 있다.
       이 그래프를 덴드로그램이라고 하며, 트리 구조로 나타낸다. 계층 군집의 장점은 k-mean처럼 엘보우나 실루엣 점수를
       활용해 적절한 k값을 찾을 필요가 없다는 점이며, 최종 클러스터의 수를 정할수도 있다.
       실전에서는 덴드로그램과 히트맵을 함께 표시해서 데이터셋을 쉽게 요약하는 방법으로 주로 사용한다.
       sklearn의 AgglomerativeClustering을 사용하면 병합 군집을 적용할 수 있지만 시각화를 위해서는 사이파이가 유리하다.
471 / 임의 형태의 데이터(반달 데이터)는 DBSCAN(밀집기반)을 사용하면 효과적으로 사용할 수 있으나, 다른 군집 알고리즘과
        마찬가지로 특성 개수가 늘어나면 차원의 저주가 생길 수 있으며, 이로 인해 일반적으로 군집을 사용하기 전에는
        PCA나 RBF커널 주성분 분석 같은 차원 축소 기법이 선행된다. 또 데이터셋을 2차원으로 축소하면 결과를
        시각화하기에 매우 유리하다.
