{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNWPr95Yk29bRog9ehqe9jw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["- TFX(TensorFlow Extended)   \n","Tensorflow 제품화를 위한 라이브러리 모음(데이터 시각화, 전처리, 서빙)   \n","https://tensorflow.org/tfx\n","- TensorFlow Hub (사전 훈련된 신경망 다운로드)   \n","https://www.tensorflow.org/hub   \n","- TensorFlow 모델 저장소   \n","https://github.com/tensorflow/models\n","- TensorFlow 리소스 페이지(TensorFlow 기반 프로젝트)   \n","https://www.tensorflow.org/resources   \n","https://github.com/jtoy/awesome-tensorflow\n","- 구현을 공개한 머신러닝 논문 사이트   \n","https://papaerswithcode.com\n","- TensorFlow Forum   \n","http://stackoverflow.com   \n","https://discuss.tensorflow.org"],"metadata":{"id":"lu7e21QbEa1s"}},{"cell_type":"code","source":["\"\"\"\n","TensorFlow 사용법\n","Tensor가 한 연산에서 다른 연산으로 흘러서 TensorFlow임.\n","기본적으로 넘파이와 유사하게 사용하지만 일부는 다르다(tf.reduce.mean/sum/max 등)\n","넘파이 배열을 텐서로 만들거나 tf.constant(a)\n","텐서를 넘파이 배열로 만들수 있다. t.numpy() / np.array(t)\n","서로 연산도 되지만 텐서와 넘파이는 기본 정밀도가 float32, float64로 각각 다르므로\n","두 개를 맞춰주어야 계산이 된다. tf.cast(t2, tf.float32)\n","\n","import tensorflow as tf\n","\n","t = tf.constant([[1,2,3],[4,5,6]]) 변경 불가능한 텐서\n","-> array([[1, 2, 3],\n","       [4, 5, 6]], dtype=int32)>\n","\n","t.shape\n","-> TensorShape([2, 3])\n","\n","t[:, 1]\n","-> array([2, 5], dtype=int32)\n","\n","t[:, 1, tf.newaxis]\n","-> array([[2],\n","       [5]], dtype=int32)\n","\n","t + 10\n","-> array([[11, 12, 13],\n","        [14, 15, 16]], dtype=int32)>\n","\n","t * 2\n","-> array([[ 2,  4,  6],\n","       [ 8, 10, 12]], dtype=int32)>\n","\n","tf.square(t)\n","-> array([[ 1,  4,  9],\n","       [16, 25, 36]], dtype=int32)>\n","\n","tf.transpose(t)\n","-> array([[1, 4],\n","       [2, 5],\n","       [3, 6]], dtype=int32)>\n","\n","\n","v = tf.Variable([[1,2,3], [4,5,6]]) # 변경 가능한 텐서\n","v[0,1].assign(42)\n","-> array([[ 1, 42,  3],\n","       [ 4,  5,  6]], dtype=int32)>\n","\n","#개별 원소나 슬라이스를 한 번에 변경 가능\n","v.scatter_nd_update(\n","    indices = [[0, 0], [1, 2]], updates = [100, 200])\n","-> array([[100,   2,   3],\n","       [  4,   5, 200]], dtype=int32)>\n","\n","v.IndexedSlices(values=[[1., 2., 3.], [4., 5., 6.]],\n","                indices=[1, 0])\n","-> array([[4., 5., 6.],\n","       [1., 2., 3.]], dtype=float32)>\n","\"\"\""],"metadata":{"id":"s6nazk9lG-S9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AGWGp4fsClLP","executionInfo":{"status":"ok","timestamp":1699920653052,"user_tz":-540,"elapsed":2,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"d82b2c78-83b3-467f-f1c7-1dae2d25db29"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=int32, numpy=\n","array([[100,   2,   3],\n","       [  4,   5, 200]], dtype=int32)>"]},"metadata":{},"execution_count":34}],"source":["import tensorflow as tf\n","\n","v = tf.Variable([[1,2,3], [4,5,6]])\n","v.scatter_nd_update(\n","    indices = [[0, 0], [1, 2]], updates = [100, 200])"]},{"cell_type":"markdown","source":["# **사용자 정의 손실**\n","```\n","후버 함수가 구현되어 있으나 이렇게 만들면 됨.\n","def create_huber(threshold = 1.0):\n","    def huber_fn(y_true, y_pred):\n","        error = y_true - y_pred\n","        is_small_error = tf.abs(error) < threshold\n","        squared_loss = tf.square(error) / 2\n","        linear_loss  = threshold * tf.abs(error) - threshold ** 2 / 2\n","        return tf.where(is_small_error, squared_loss, linear_loss)\n","    return huber_fn\n","\n","model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[\"mae\"])\n","model.fit(X_train_scaled, y_train, epochs=2,\n","          validation_data=(X_valid_scaled, y_valid))\n","\n","모델 저장은 문제 없는데, 불러올때는 함수까지 같이 구현해야하고.   \n","그 전에 사용했던 threshold 값이 저장되지 않으므로.\n","Class로 구현해서 저장하던지. 아래와 같이 모델 이름에 남겨서 기억해야함\n","model.save(\"my_model_with_a_custom_loss_threshold_2\")\n","model = tf.keras.models.load_model(\"my_model_with_a_custom_loss_threshold_2\",\n","                                   custom_objects={\"huber_fn\": create_huber(2.0)})\n","```\n","```\n","class HuberLoss(tf.keras.losses.Loss):\n","    def __init__(self, threshold=1.0, **kwargs):\n","        self.threshold = threshold\n","        super().__init__(**kwargs)\n","\n","    def call(self, y_true, y_pred):\n","        error = y_true - y_pred\n","        is_small_error = tf.abs(error) < self.threshold\n","        squared_loss = tf.square(error) / 2\n","        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n","        return tf.where(is_small_error, squared_loss, linear_loss)\n","\n","    def get_config(self):\n","        base_config = super().get_config()\n","        return {**base_config, \"threshold\": self.threshold}\n","\n","model.compile(loss=HuberLoss(2.), optimizer=\"nadam\", metrics=[\"mae\"])\n","\n","이렇게 클래스로 구현하면 get_config 함수에서 저장된 threshold 값을 불러옴\n","model.save(\"my_model_with_a_custom_loss_class\")\n","model = tf.keras.models.load_model(\"my_model_with_a_custom_loss_class\",\n","                                   custom_objects={\"HuberLoss\": HuberLoss})\n","```"],"metadata":{"id":"Dx4qtfoLSQwl"}},{"cell_type":"markdown","source":["#**기타 사용자 정의 기능**\n","```\n","간단하게 활성화 함수, 초기화 방법, 규제, 제한 등을 구현할 수 있다.\n","def my_softplus(z):\n","    return tf.math.log(1 + tf.exp(-tf.abs(z))) + tf.maximum(0, z)\n","    \n","def my_glorot_initializer(shape, dtype=tf.float32):\n","    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n","    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n","\n","def my_l1_regularizer(weights):\n","    return tf.reduce_sum(tf.abs(0.01 * weights))\n","\n","def my_positive_weights(weights):  # 반환 값은 tf.nn.relu(weights)와 같음.\n","    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n","\n","layer = tf.keras.layers.Dense(1, activation=my_softplus,\n","                              kernel_initializer=my_glorot_initializer,\n","                              kernel_regularizer=my_l1_regularizer,\n","                              kernel_constraint=my_positive_weights)\n","```"],"metadata":{"id":"s_yRIbEmTErX"}},{"cell_type":"markdown","source":["# **사용자 정의 층**\n","```\n","아주 간단한 구현\n","# activation = 'exponential'과 같다\n","exponential_layer = tf.keras.layers.Lambda(lambda x: tf.exp(x))\n","```\n","```\n","#Dense 레이어와 같은 역할을 하는 사용자 정의 층.\n","\n","class MyDense(tf.keras.layers.Layer): #layers.Layer를 상속한다.\n","    def __init__(self, units, activation=None, **kwargs):\n","        super().__init__(**kwargs)\n","        self.units = units\n","        self.activation = tf.keras.activations.get(activation)\n","\n","    def build(self, batch_input_shape):\n","        self.kernel = self.add_weight(\n","            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n","            initializer=\"he_normal\")\n","        self.bias = self.add_weight(\n","            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n","        super().build(batch_input_shape)  # 마지막에 호출해야 합니다.\n","\n","    def call(self, X):\n","        return self.activation(X @ self.kernel + self.bias)\n","\n","    def compute_output_shape(self, batch_input_shape):\n","        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n","\n","    def get_config(self):\n","        base_config = super().get_config()\n","        return {**base_config, \"units\": self.units,\n","                \"activation\": tf.keras.activations.serialize(self.activation)}\n","\n","#로드는 이렇게 한다.\n","model = tf.keras.models.load_model(\"my_model_with_a_custom_layer\",\n","                                   custom_objects={\"MyDense\": MyDense})\n","model.fit(X_train_scaled, y_train, epochs=2,\n","          validation_data=(X_valid_scaled, y_valid))\n","```\n","```\n","함수형API나 서브클래스 API에서 사용하는 다중 입출력 레이어\n","아래는 2개의 입력을 받고 3개의 출력을 만든다.\n","class MyMultiLayer(tf.keras.layers.Layer):\n","    def call(self, X):\n","        X1, X2 = X\n","        print(\"X1.shape: \", X1.shape ,\" X2.shape: \", X2.shape)  # extra code\n","        return X1 + X2, X1 * X2, X1 / X2\n","\n","    def compute_output_shape(self, batch_input_shape):\n","        batch_input_shape1, batch_input_shape2 = batch_input_shape\n","        return [batch_input_shape1, batch_input_shape1, batch_input_shape1]\n","\n","#예시\n","inputs1 = tf.keras.layers.Input(shape=[2])\n","inputs2 = tf.keras.layers.Input(shape=[2])\n","MyMultiLayer()((inputs1, inputs2))\n","```\n","```\n","Training 매개변수를 추가해 훈련과 테스트때 다르게 동작하는 층을 만들수도 있다.\n","class MyGaussianNoise(tf.keras.layers.Layer):\n","    def __init__(self, stddev, **kwargs):\n","        super().__init__(**kwargs)\n","        self.stddev = stddev\n","\n","    def call(self, X, training=None):\n","        if training:\n","            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n","            return X + noise\n","        else:\n","            return X\n","\n","    def compute_output_shape(self, batch_input_shape):\n","        return batch_input_shape\n","\n","model = tf.keras.Sequential([\n","    MyGaussianNoise(stddev=1.0, input_shape=input_shape),\n","    tf.keras.layers.Dense(30, activation=\"relu\",\n","                          kernel_initializer=\"he_normal\"),\n","    tf.keras.layers.Dense(1)\n","])\n","```"],"metadata":{"id":"dZDKGY8UWaOU"}},{"cell_type":"markdown","source":["# **사용자 정의 모델**\n","```\n","# 기본적인 잔차블록 모델\n","class ResidualBlock(tf.keras.layers.Layer):\n","    def __init__(self, n_layers, n_neurons, **kwargs):\n","        super().__init__(**kwargs)\n","        self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"relu\",\n","                                             kernel_initializer=\"he_normal\")\n","                       for _ in range(n_layers)]\n","\n","    def call(self, inputs):\n","        Z = inputs\n","        for layer in self.hidden:\n","            Z = layer(Z)\n","        return inputs + Z\n","\n","# 서브클래싱 API를 통해 잔차블록 모델 정의\n","class ResidualRegressor(tf.keras.Model):\n","    def __init__(self, output_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.hidden1 = tf.keras.layers.Dense(30, activation=\"relu\",\n","                                             kernel_initializer=\"he_normal\")\n","        self.block1 = ResidualBlock(2, 30)\n","        self.block2 = ResidualBlock(2, 30)\n","        self.out = tf.keras.layers.Dense(output_dim)\n","\n","    def call(self, inputs):\n","        Z = self.hidden1(inputs)\n","        for _ in range(1 + 3):\n","            Z = self.block1(Z)\n","        Z = self.block2(Z)\n","        return self.out(Z)\n","\n","# 모델을 사용하고 저장하는 과정\n","model = ResidualRegressor(1)\n","model.compile(loss=\"mse\", optimizer=\"nadam\")\n","history = model.fit(X_train_scaled, y_train, epochs=2)\n","score = model.evaluate(X_test_scaled, y_test)\n","model.save(\"my_custom_model\")\n","```"],"metadata":{"id":"AJV49lvkXw6c"}},{"cell_type":"markdown","source":["# **모델 구성 요소에 기반한 손실과 지표**\n","```\n","#일반적인 모델은 레이블과 예측을 기반으로 하지만. 은닉층의 가중치나\n","활성화 함수와 같은 모델의 구성 요소에 기반한 손실을 정의할 수도 있다.\n","이런 손실은 보조 규제로 사용되거나 모델의 내부 상황을 모니터링 할 수 있게 한다.\n","class ReconstructingRegressor(tf.keras.Model):\n","    def __init__(self, output_dim, **kwargs): #생성자\n","        super().__init__(**kwargs)\n","        self.hidden = [tf.keras.layers.Dense(30, activation=\"relu\",\n","                                             kernel_initializer=\"he_normal\")\n","                       for _ in range(5)]\n","        self.out = tf.keras.layers.Dense(output_dim)\n","        self.reconstruction_mean = tf.keras.metrics.Mean(\n","            name=\"reconstruction_error\")\n","\n","    def build(self, batch_input_shape): #입력 차원에 맞게 재구성 층 구현\n","        n_inputs = batch_input_shape[-1]\n","        self.reconstruct = tf.keras.layers.Dense(n_inputs)\n","        #self.built = True  # super().build(batch_input_shape) 에러 발생시 사용\n","\n","    def call(self, inputs, training=None): #재구성 손실 구현\n","        Z = inputs\n","        for layer in self.hidden:\n","            Z = layer(Z)\n","        reconstruction = self.reconstruct(Z)\n","        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n","        self.add_loss(0.05 * recon_loss)\n","        if training:\n","            result = self.reconstruction_mean(recon_loss)\n","            self.add_metric(result)\n","        return self.out(Z)\n","```"],"metadata":{"id":"B1m0DAB_a9Ql"}},{"cell_type":"markdown","source":["# **Gradient Tape**\n","```\n","def f(w1, w2):\n","    return 3 * w1 ** 2 + 2 * w1 * w2\n","\n","w1, w2 = tf.Variable(5.), tf.Variable(3.)\n","with tf.GradientTape() as tape:\n","    z = f(w1, w2)\n","\n","gradients = tape.gradient(z, [w1, w2])\n","gradients\n","-> [<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n","    <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]\n","```\n","```\n","그레이디언트를 전파하고 싶지 않은 변수는 다음과 같이 한다.\n","def f(w1, w2):\n","    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n","\n","with tf.GradientTape() as tape:\n","    z = f(w1, w2)  # stop_gradient()를 사용하지 않은 경우와 동일한 결과\n","\n","gradients = tape.gradient(z, [w1, w2])\n","gradients\n","-> [<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]\n","```\n","```\n","#이렇게 변수가 아닌 객체를 전달하면 None을 반환한다\n","c1, c2 = tf.constant(5.), tf.constant(3.)\n","with tf.GradientTape() as tape:\n","    z = f(c1, c2)\n","\n","gradients = tape.gradient(z, [c1, c2])\n","gradients\n","-> [None, None]\n","\n","#그러나 watch로 기록을 명시적으로 알리면 제대로 출력한다\n","with tf.GradientTape() as tape:\n","    tape.watch(c1)\n","    tape.watch(c2)\n","    z = f(c1, c2)\n","\n","gradients = tape.gradient(z, [c1, c2])\n","gradients\n","-> [<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n","    <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]\n","```"],"metadata":{"id":"7Ucxo1R7hT7b"}}]}