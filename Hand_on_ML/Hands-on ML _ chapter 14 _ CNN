{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["kOkPuI-4vYcW","BEyIayb1vb0I"],"gpuType":"T4","authorship_tag":"ABX9TyPtRQY61Eb92UIqLIvMAR2f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"xk4FVG35PyDo"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import numpy as np\n","\n","plt.rc('font', size=14)\n","plt.rc('axes', labelsize=14, titlesize=14)\n","plt.rc('legend', fontsize=14)\n","plt.rc('xtick', labelsize=10)\n","plt.rc('ytick', labelsize=10)\n","\n","import sys\n","# 코랩의 경우 나눔 폰트를 설치합니다.\n","if 'google.colab' in sys.modules:\n","    !sudo apt-get -qq -y install fonts-nanum\n","    import matplotlib.font_manager as fm\n","    font_files = fm.findSystemFonts(fontpaths=['/usr/share/fonts/truetype/nanum'])\n","    for fpath in font_files:\n","        fm.fontManager.addfont(fpath)\n","\n","# 나눔 폰트를 사용합니다.\n","import matplotlib\n","\n","matplotlib.rc('font', family='NanumBarunGothic')\n","matplotlib.rcParams['axes.unicode_minus'] = False"]},{"cell_type":"markdown","source":["FCN : https://arxiv.org/pdf/1411.4038.pdf"],"metadata":{"id":"lCz4G-4qiVwp"}},{"cell_type":"markdown","source":["# **기본적인 CNN 구조**"],"metadata":{"id":"kOkPuI-4vYcW"}},{"cell_type":"code","source":["# 기본적인 CNN 구조\n","from functools import partial\n","import tensorflow as tf\n","import numpy as np\n","\n","mnist = tf.keras.datasets.fashion_mnist.load_data()\n","(X_train_full, y_train_full), (X_test, y_test) = mnist\n","#(60000, 28 ,28) -> (60000, 28, 28, 1(흑백))으로 만든다.\n","X_train_full = np.expand_dims(X_train_full, axis=-1).astype(np.float32) / 255\n","X_test = np.expand_dims(X_test.astype(np.float32), axis=-1) / 255\n","X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n","y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n","\n","tf.random.set_seed(42)  # 추가 코드 - 재현성 보장"],"metadata":{"id":"J5C9ew95X_D2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","코드 설명\n","1. 아래 처럼 partial함수를 사용하면 DefaultConv2D를 쓸때마다\n","   tf.keras.layers.Conv2D에 나머지 하이퍼 파라미터들을 자동으로 적용해준다.\n","\n","2. 활성화 함수는 relu를 사용하고, kernel초기화도 relu와 어울리는 he_normal로\n","   적용한다\n","3. 커널의 크기를 크게 하는건 바람직 하지 않지만, 예외적으로 첫 번째 커널에서는\n","   큰 값을 사용할 수 있으며(5x5 , 7x7)이때는 너무 많은 정보를 잃지 않으면서\n","   공간 방향 차원(이미지 크기) 줄일 수 있고. 처음에는 차원이 3개이므로(R,G,B) 비용도\n","   크게 들지 않는다.\n","4. MaxPool2D층으로 크기를 절반으로 줄이고 필터 크기(깊이)를 2배 늘리는 전략을\n","   일반적으로 사용한다.\n","5. 층 마지막에는 0~9까지의 숫자로 확률을 표현해야 하기 때문에\n","   Flatten층으로 1D 배열로 펼친 뒤, Dropout을 거치며 출력층까지 도달한다.\n","\"\"\"\n","\n","tf.keras.backend.clear_session()\n","\n","model = tf.keras.Sequential([\n","    tf.keras.layers.BatchNormalization(),\n","    tf.keras.layers.Conv2D(128, kernel_size=3, padding=\"same\",\n","                           activation=\"relu\", kernel_initializer=\"he_normal\"),\n","    tf.keras.layers.Conv2D(64, kernel_size=3, padding=\"same\",\n","                           activation=\"relu\", kernel_initializer=\"he_normal\"),\n","    tf.keras.layers.MaxPool2D(),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dropout(0.25),\n","    tf.keras.layers.Dense(128, activation=\"relu\",\n","                          kernel_initializer=\"he_normal\"),\n","    tf.keras.layers.Dropout(0.5),\n","    tf.keras.layers.Dense(10, activation=\"softmax\")\n","])\n","\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n","              metrics=[\"accuracy\"])\n","history = model.fit(X_train, y_train, epochs=10,\n","                    validation_data=(X_valid, y_valid))\n","score = model.evaluate(X_test, y_test)\n","\n","#tf.keras.layers.BatchNormalization(),\n","#tf.keras.layers.GlobalAveragePooling2D(),"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P6GDhtRxNdod","executionInfo":{"status":"ok","timestamp":1700039366659,"user_tz":-540,"elapsed":145275,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"8af10698-9221-4562-f453-c09eba9cc1c5"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","1719/1719 [==============================] - 18s 8ms/step - loss: 0.4825 - accuracy: 0.8338 - val_loss: 0.2826 - val_accuracy: 0.8924\n","Epoch 2/10\n","1719/1719 [==============================] - 14s 8ms/step - loss: 0.3107 - accuracy: 0.8859 - val_loss: 0.2434 - val_accuracy: 0.9094\n","Epoch 3/10\n","1719/1719 [==============================] - 14s 8ms/step - loss: 0.2618 - accuracy: 0.9027 - val_loss: 0.2171 - val_accuracy: 0.9180\n","Epoch 4/10\n","1719/1719 [==============================] - 13s 8ms/step - loss: 0.2314 - accuracy: 0.9147 - val_loss: 0.2146 - val_accuracy: 0.9166\n","Epoch 5/10\n","1719/1719 [==============================] - 14s 8ms/step - loss: 0.2027 - accuracy: 0.9243 - val_loss: 0.1916 - val_accuracy: 0.9250\n","Epoch 6/10\n","1719/1719 [==============================] - 14s 8ms/step - loss: 0.1818 - accuracy: 0.9337 - val_loss: 0.2018 - val_accuracy: 0.9268\n","Epoch 7/10\n","1719/1719 [==============================] - 14s 8ms/step - loss: 0.1666 - accuracy: 0.9385 - val_loss: 0.1902 - val_accuracy: 0.9298\n","Epoch 8/10\n","1719/1719 [==============================] - 14s 8ms/step - loss: 0.1503 - accuracy: 0.9444 - val_loss: 0.1931 - val_accuracy: 0.9326\n","Epoch 9/10\n","1719/1719 [==============================] - 14s 8ms/step - loss: 0.1373 - accuracy: 0.9485 - val_loss: 0.1918 - val_accuracy: 0.9338\n","Epoch 10/10\n","1719/1719 [==============================] - 14s 8ms/step - loss: 0.1274 - accuracy: 0.9528 - val_loss: 0.1918 - val_accuracy: 0.9322\n","313/313 [==============================] - 1s 4ms/step - loss: 0.2221 - accuracy: 0.9274\n"]}]},{"cell_type":"markdown","source":["# **# 케라스로 ResNet-34 CNN 구현하기**"],"metadata":{"id":"BEyIayb1vb0I"}},{"cell_type":"code","source":["# 케라스로 ResNet-34 CNN 구현하기\n","DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, strides=1,\n","                        padding=\"same\", kernel_initializer=\"he_normal\",\n","                        use_bias=False)\n","\n","#\n","class ResidualUnit(tf.keras.layers.Layer):\n","    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n","        super().__init__(**kwargs)\n","        self.activation = tf.keras.activations.get(activation)\n","        self.main_layers = [\n","            # 합성곱 -> 배치정규화 -> Relu -> 합성곱 -> 배치정규화\n","            DefaultConv2D(filters, strides=strides),\n","            tf.keras.layers.BatchNormalization(),\n","            self.activation,\n","            DefaultConv2D(filters),\n","            tf.keras.layers.BatchNormalization()\n","        ]\n","        self.skip_layers = []\n","        # filter 크기가 바뀌면 합성곱(Kernel_size = 1) -> 배치정규화 추가\n","        if strides > 1:\n","            self.skip_layers = [\n","                DefaultConv2D(filters, kernel_size=1, strides=strides),\n","                tf.keras.layers.BatchNormalization()\n","            ]\n","\n","    def call(self, inputs):\n","        Z = inputs\n","        for layer in self.main_layers:\n","            Z = layer(Z)\n","        skip_Z = inputs\n","        for layer in self.skip_layers:\n","            skip_Z = layer(skip_Z)\n","        return self.activation(Z + skip_Z)"],"metadata":{"id":"rU__VwZPpwNm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = tf.keras.Sequential([\n","    DefaultConv2D(64, kernel_size=7, strides=2, input_shape=[224, 224, 3]),\n","    tf.keras.layers.BatchNormalization(),\n","    tf.keras.layers.Activation(\"relu\"),\n","    tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"),\n","])\n","prev_filters = 64\n","for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n","    strides = 1 if filters == prev_filters else 2\n","    model.add(ResidualUnit(filters, strides=strides))\n","    prev_filters = filters\n","\n","model.add(tf.keras.layers.GlobalAvgPool2D())\n","model.add(tf.keras.layers.Flatten())\n","model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"],"metadata":{"id":"DQi1mrJ0p0Ah"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 물론 이렇게 불러오면 된다.\n","from sklearn.datasets import load_sample_images\n","\n","model = tf.keras.applications.ResNet50(weights=\"imagenet\")\n","\n","#사용 예시\n","images = load_sample_images()[\"images\"] #ResNet50모델은 224*224입력을 기대한다.\n","images_resized = tf.keras.layers.Resizing(height=224, width=224,\n","                                          crop_to_aspect_ratio=True)(images)\n","\n","# 이렇게 모델을 불러와 사용할때 preprocess_input를 사용하면\n","# 알아서 전처리까지 다 해준다(세팅까지 저장되어 있으므로).\n","inputs = tf.keras.applications.resnet50.preprocess_input(images_resized)\n","Y_proba = model.predict(inputs)\n","\n","top_K = tf.keras.applications.resnet50.decode_predictions(Y_proba, top=3)\n","for image_index in range(len(images)):\n","    print(f\"image #{image_index}\")\n","    for class_id, name, y_proba in top_K[image_index]:\n","        print(f\"  {class_id} - {name:12s} {y_proba:.2%}\")\n","\n","\"\"\"\n","실제로는 palace와 dahlia이지만 dahlia는 imagenet클래스에 없고\n","vase(꽃병)이나 daisy도 유사한 답변임.\n","image #0\n","  n03877845 - palace       54.69%\n","  n03781244 - monastery    24.71%\n","  n02825657 - bell_cote    18.55%\n","image #1\n","  n04522168 - vase         32.67%\n","  n11939491 - daisy        17.82%\n","  n03530642 - honeycomb    12.04%\n","\"\"\""],"metadata":{"id":"cUJ7CXFfrQY6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **사전 훈련된 모델을 사용한 전이 학습**"],"metadata":{"id":"5K3n3byQvd0-"}},{"cell_type":"code","source":["# Xception 모델을 사용한 전이 학습"],"metadata":{"id":"KDb2Ii8evQem"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["```\n","dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\n","dataset_size = info.splits[\"train\"].num_examples\n","class_names = info.features[\"label\"].names\n","n_classes = info.features[\"label\"].num_classes\n","\n","dataset_size = 3670\n","class_names = ['dandelion', 'daisy', 'tulips', 'sunflowers', 'roses']\n","n_classes = 5\n","```"],"metadata":{"id":"Lc1rgMd-vrlt"}},{"cell_type":"code","source":["import tensorflow_datasets as tfds\n","\n","#먼저 샘플 크기, 클래스 종류와 개수 등을 변수로 저장해놓는다.\n","dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\n","dataset_size = info.splits[\"train\"].num_examples\n","class_names = info.features[\"label\"].names\n","n_classes = info.features[\"label\"].num_classes\n","\n","#파일을 다시 불러오고 데이셋을 나눈다.\n","#이 데이터셋에는 test셋이 없으므로 훈련세트를 75 / 15 / 10으로 나눈다\n","train_set_raw, valid_set_raw, test_set_raw = tfds.load(\n","    \"tf_flowers\",\n","    split=[\"train[:75%]\", \"train[75%:90%]\", \"train[90%:]\"],\n","    as_supervised=True)\n","\n","#세 데이터 이미지를 낱개로 포함하고 있으므로\n","#배치로 묶어야 하지만 그 전에 전처리를 해야한다.\n","\n","#1. 이미지 크기를 조정하고, Xception의 preprocess_input을 적용한다\n","batch_size = 32\n","preprocess = tf.keras.Sequential([\n","    tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True),\n","    tf.keras.layers.Lambda(tf.keras.applications.xception.preprocess_input)\n","])\n","\n","#2. 이미지를 전처리하고 배치로 묶는다. 훈련세트는 섞고 prefetch 한다.\n","train_set = train_set_raw.map(lambda X, y: (preprocess(X), y))\n","train_set = train_set.shuffle(1000, seed=42).batch(batch_size).prefetch(1)\n","valid_set = valid_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)\n","test_set = test_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)\n","\n","#3. 데이터 샘플의 개수가 많지 않으므로(3670) 다음과 같은 증식을 수행한다.\n","data_augmentation = tf.keras.Sequential([\n","    tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=42),\n","    tf.keras.layers.RandomRotation(factor=0.05, seed=42),\n","    tf.keras.layers.RandomContrast(factor=0.2, seed=42)\n","])"],"metadata":{"id":"OhnsQzcov0Nn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#4. 모델을 불러온다. 이때 마지막 출력층(전역풀링, 밀집출력)은 빼고 가져온다.\n","#   (내가 쓰고자 하는 class는 다르기 때문에)\n","tf.random.set_seed(42)\n","base_model = tf.keras.applications.xception.Xception(weights=\"imagenet\",\n","                                                     include_top=False)\n","avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n","output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n","model = tf.keras.Model(inputs=base_model.input, outputs=output)"],"metadata":{"id":"Gx4EAhxHxIeR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#5. 바뀐 출력층으로 인해 모델의 훈련된 가중치가 망가지는것을 방지하기 위해\n","#   모든 층을 고정하고 몇 에포크 훈련시킨다.\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n","optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n","              metrics=[\"accuracy\"])\n","history = model.fit(train_set, validation_data=valid_set, epochs=5)"],"metadata":{"id":"Mo5ehWkCxx5p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 참고(층 전체를 보기 좋게 불러옴)\n","for indices in zip(range(33), range(33, 66), range(66, 99), range(99, 132)):\n","    for idx in indices:\n","        print(f\"{idx:3}: {base_model.layers[idx].name:22}\", end=\"\")\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n1CUmjUYzkC_","executionInfo":{"status":"ok","timestamp":1700016369770,"user_tz":-540,"elapsed":2,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"50cf19bc-134c-4a6f-8138-7b0f2de24ae2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  0: input_2                33: block4_pool            66: block8_sepconv1_act    99: block11_sepconv2_act  \n","  1: block1_conv1           34: batch_normalization_2  67: block8_sepconv1       100: block11_sepconv2      \n","  2: block1_conv1_bn        35: add_2                  68: block8_sepconv1_bn    101: block11_sepconv2_bn   \n","  3: block1_conv1_act       36: block5_sepconv1_act    69: block8_sepconv2_act   102: block11_sepconv3_act  \n","  4: block1_conv2           37: block5_sepconv1        70: block8_sepconv2       103: block11_sepconv3      \n","  5: block1_conv2_bn        38: block5_sepconv1_bn     71: block8_sepconv2_bn    104: block11_sepconv3_bn   \n","  6: block1_conv2_act       39: block5_sepconv2_act    72: block8_sepconv3_act   105: add_9                 \n","  7: block2_sepconv1        40: block5_sepconv2        73: block8_sepconv3       106: block12_sepconv1_act  \n","  8: block2_sepconv1_bn     41: block5_sepconv2_bn     74: block8_sepconv3_bn    107: block12_sepconv1      \n","  9: block2_sepconv2_act    42: block5_sepconv3_act    75: add_6                 108: block12_sepconv1_bn   \n"," 10: block2_sepconv2        43: block5_sepconv3        76: block9_sepconv1_act   109: block12_sepconv2_act  \n"," 11: block2_sepconv2_bn     44: block5_sepconv3_bn     77: block9_sepconv1       110: block12_sepconv2      \n"," 12: conv2d_5               45: add_3                  78: block9_sepconv1_bn    111: block12_sepconv2_bn   \n"," 13: block2_pool            46: block6_sepconv1_act    79: block9_sepconv2_act   112: block12_sepconv3_act  \n"," 14: batch_normalization    47: block6_sepconv1        80: block9_sepconv2       113: block12_sepconv3      \n"," 15: add                    48: block6_sepconv1_bn     81: block9_sepconv2_bn    114: block12_sepconv3_bn   \n"," 16: block3_sepconv1_act    49: block6_sepconv2_act    82: block9_sepconv3_act   115: add_10                \n"," 17: block3_sepconv1        50: block6_sepconv2        83: block9_sepconv3       116: block13_sepconv1_act  \n"," 18: block3_sepconv1_bn     51: block6_sepconv2_bn     84: block9_sepconv3_bn    117: block13_sepconv1      \n"," 19: block3_sepconv2_act    52: block6_sepconv3_act    85: add_7                 118: block13_sepconv1_bn   \n"," 20: block3_sepconv2        53: block6_sepconv3        86: block10_sepconv1_act  119: block13_sepconv2_act  \n"," 21: block3_sepconv2_bn     54: block6_sepconv3_bn     87: block10_sepconv1      120: block13_sepconv2      \n"," 22: conv2d_6               55: add_4                  88: block10_sepconv1_bn   121: block13_sepconv2_bn   \n"," 23: block3_pool            56: block7_sepconv1_act    89: block10_sepconv2_act  122: conv2d_8              \n"," 24: batch_normalization_1  57: block7_sepconv1        90: block10_sepconv2      123: block13_pool          \n"," 25: add_1                  58: block7_sepconv1_bn     91: block10_sepconv2_bn   124: batch_normalization_3 \n"," 26: block4_sepconv1_act    59: block7_sepconv2_act    92: block10_sepconv3_act  125: add_11                \n"," 27: block4_sepconv1        60: block7_sepconv2        93: block10_sepconv3      126: block14_sepconv1      \n"," 28: block4_sepconv1_bn     61: block7_sepconv2_bn     94: block10_sepconv3_bn   127: block14_sepconv1_bn   \n"," 29: block4_sepconv2_act    62: block7_sepconv3_act    95: add_8                 128: block14_sepconv1_act  \n"," 30: block4_sepconv2        63: block7_sepconv3        96: block11_sepconv1_act  129: block14_sepconv2      \n"," 31: block4_sepconv2_bn     64: block7_sepconv3_bn     97: block11_sepconv1      130: block14_sepconv2_bn   \n"," 32: conv2d_7               65: add_5                  98: block11_sepconv1_bn   131: block14_sepconv2_act  \n"]}]},{"cell_type":"code","source":["# 56번째 층 이후로 동결을 해제하고 다시 훈련해본다.\n","# 동결을 해제할때는 반드시 다시 컴파일 해야하고, 가중치가 훼손되는 것을\n","# 막기 위해서 더 낮은 학습률을 사용해야 한다.\n","\n","for layer in base_model.layers[56:]:\n","    layer.trainable = True\n","\n","optimizer = tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9)\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n","              metrics=[\"accuracy\"])\n","history = model.fit(train_set, validation_data=valid_set, epochs=10)"],"metadata":{"id":"3fZUiSdtzUPz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.evaluate(test_set)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j-jXeQtU3ZOC","executionInfo":{"status":"ok","timestamp":1700016309864,"user_tz":-540,"elapsed":3035,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"59090946-654f-454b-cdb4-f532e323c6f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["12/12 [==============================] - 2s 129ms/step - loss: 0.2346 - accuracy: 0.9373\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.23464596271514893, 0.9373297095298767]"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["CNN 응용 : 객체 탐지, 객체 추정, 시멘틱 분할, FCN, YOLO"],"metadata":{"id":"-AXWiMJIWy2n"}}]}