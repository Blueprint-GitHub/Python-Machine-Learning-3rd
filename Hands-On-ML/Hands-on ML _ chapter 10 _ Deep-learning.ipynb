{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["j-KgYnBx1hd7","M959i_D81ksv","7b7XxvtH2ZO3","MNfaHfw4_r29","8xakJ1RHLsP0"],"gpuType":"T4","authorship_tag":"ABX9TyOb6bR58Lpyyxy+GaZJP1pk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["1.   학습률 : 일반적으로 훈련 알고리즘이 발산하는 학습률의 절반 or 학습률을 올릴때 손실이 발생하는 지점의 1/10 정도가 적절한 학습률이다. 학습률은 다른\n","하이퍼 파라미터에 대해 많은 영향을 받으므로, 하이퍼 파라미터를 수정했다면\n","학습률도 수정해야한다.\n","\n","2.   배치 크기 : GPU RAM에 맞는 가장 큰 배치 크기를 사용하라고 조언하는 사람이\n","많지만 한 32개 정도가 처음에 시도해보기 좋은 배치 크기이다. 학습률 예열 같은\n","기법을 사용하면 배치크기를 8192개 까지 늘리는 등의 기교를 부릴 수 있다.\n","compile메서드의 steps_per_excution을 1 이상으로 지정하면 한 번에 여러 배치를\n","처리해 GPU를 최대한으로 활용하여 훈련 속도를 빠르게 할 수 있다.\n","\n","3. 반복 횟수 : 반복 횟수를 튜닝하기보다는 조기 종료 콜백을 사용한다.\n","\n","4. 옵티마이저 : 각 데이터셋에 적절한 옵티마이저를 선택해야한다.\n","\n","5. 가중치 감쇠(weight decay) = 1e-4가 일단은 시작하기 좋은 값처럼 보임\n","\n","하이퍼 파라미터 튜닝 모범 사례 arxiv 1803.09820 (https://homl.info/1cycle)\n"],"metadata":{"id":"5ypYQ4WphxsM"}},{"cell_type":"markdown","source":["# 딥러닝 모델 Pipeline"],"metadata":{"id":"j-KgYnBx1hd7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"P9DJR1NCnCUU"},"outputs":[],"source":["import tensorflow as tf\n","\n","fashion_minst = tf.keras.datasets.fashion_mnist.load_data()\n","(X_train_full, y_train_full), (X_test, y_test) = fashion_minst\n","X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n","X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]"]},{"cell_type":"code","source":["X_train, X_valid, X_test = X_train / 255. , X_valid / 255., X_test / 255. # 전처리"],"metadata":{"id":"PW58MTnFoagr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n","               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"],"metadata":{"id":"HICC2s8WomLw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 가장 기본적인 모델 쌓기\n","from tensorflow import keras\n","from keras import layers\n","\n","tf.random.set_seed(42)\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Input(shape=(28, 28)))\n","model.add(tf.keras.layers.Flatten())\n","model.add(tf.keras.layers.Dense(300, activation=\"relu\"))\n","model.add(tf.keras.layers.Dense(100, activation=\"relu\"))\n","model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n","\n","#아래와 같다. 시퀀셜 API\n","tf.keras.backend.clear_session()\n","tf.random.set_seed(42)\n","\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Flatten(input_shape=[28, 28]),\n","    tf.keras.layers.Dense(300, activation=\"relu\"),\n","    tf.keras.layers.Dense(100, activation=\"relu\"),\n","    tf.keras.layers.Dense(10, activation=\"softmax\")\n","])\n","\n","#또는 이렇게도 쓸 수 있다. 함수형 API 라고 함.\n","tf.keras.backend.clear_session()\n","tf.random.set_seed(42)\n","\n","inputs = keras.Input(shape = (28,28))\n","x = layers.Flatten()(inputs)\n","x = layers.Dense(300, activation = 'relu')(x)\n","x = layers.Dense(100, activation = 'relu')(x)\n","outputs = layers.Dense(10, activation = 'softmax')(x)\n","\n","model = keras.Model(inputs, outputs)"],"metadata":{"id":"K19OVKjao2-k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","# 모델을 쌓고나면 컴파일 해야한다\n","loss = binary_crossentropy : 이진 분류\n","       sparse_categorical_crossentropy : 다중 분류(원-핫 벡터가 아닐때)\n","       categorical_crossentropy : 다중 분류 (원-핫 벡터 일때)\n","\n","optimizer = rmsprop (Root Mean Square Propagation) : 기본값, 적응적인 학습률\n","            sgd (Stochastic Gradient Descent) : 확률적 경사 하강법\n","            adma (Adaptive Moment Estimation) : SGD + Rmsprop\n","            adagrad (Adaptive Gradient Algorithm) : 각 파라미터에 대해 개별적 학습률\n","            Nadam (Nesterov-accelerated Adaptive Moment Estimation)\n","\n","metrics = Accuracy : 올바르게 예측한 샘플의 비율\n","          Precision : 양성 클래스로 예측한 샘플 중 실제 양성의 비율\n","          Recall : 양성 클래스 샘플 중 양성으로 예측한 샘플의 비율\n","          F1 Score : 정밀도와 재현율의 조화 평균\n","          AUC : ROC 곡선 아래의 면적 (1이 최대)\n","\n","metrcis 추가 지표 : binary_crossentropy,\n","                    categorical_crossentropy,\n","                    mean_squared_error\n","                    True Positives, False Positives, True Negatives, False Negatives\n","\"\"\"\n","\n","\n","\"\"\"\n","# 추가 코드 - 이 셀은 아래와 같다.\n","model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n","              optimizer=tf.keras.optimizers.SGD(),\n","              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n","\"\"\"\n","model.compile(loss = 'sparse_categorical_crossentropy',\n","              optimizer = 'sgd',\n","              metrics = ['accuracy'])\n"],"metadata":{"id":"jvqYJb4MrrYL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","# 모델 훈련\n","class_weight = 각 feature 마다 weight를 할당할 수 있다. (기본 1, 0은 제외를 의미)\n","               딕셔너리 형태로 제공한다.\n","sample_weight = 각 Sample 마다 weight를 할당할 수 있다.\n","batch_size = 기본은 32로 전체 샘플을 32개로 쪼개서 훈련하는 것을 의미함(미니배치)\n","\n","validation_data = 검증세트를 전달할 수도 있고, validation_split = 0.1처럼 지정해\n","                  X_train, y_train의 10%를 떼서 검증 세트로 쓸 수도 있다.\n","                  따로 나누는 것 보다 이게 더 좋은 것 같음.\n","\n","callbacks = 콜백 리스트를 따로 만들어서 전달하는게 효율적\n","\n","모델을 훈련한 후 history.history에는 loss ,val_loss(validation을 지정한 경우),\n","설정한 메트릭에 따라 'accuracy', 'precision' 등이 들어있다.\n","\"\"\"\n","history = model.fit(X_train, y_train, epochs = 30,\n","                    validation_data = (X_valid, y_valid))"],"metadata":{"id":"bbc35-43uO3U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#모델을 훈련한 뒤에는 테스트 점수를 계산하면 된다.\n","#[0]에는 compile에서 지정한 loss가, [1]에는 metrics가 들어있다\n","model.evaluate(X_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Xg53eFe0KQj","executionInfo":{"status":"ok","timestamp":1699764094196,"user_tz":-540,"elapsed":1019,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"f426ee8f-efc3-47e2-c907-1fd486956555"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["313/313 [==============================] - 1s 2ms/step - loss: 0.3234 - accuracy: 0.8859\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.3233807384967804, 0.8859000205993652]"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["\"\"\"\n","콜백(Callbacks)\n","ModelCheckPoint와 Early Stopping을 주로 사용하며\n","아래와 같이 구현한다.\n","\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","callbacks = [ModelCheckpoint(\"my_checkpoints\", save_weight_only = True,\n","                             save_best_only = True),\n","             EarlyStopping(patience = 10, restore_best_weights = True)\n","]\n","\n","history = model.fit(X_train, y_train, epochs = 30,\n","                    validation_data = (X_valid, y_valid),\n","                    callbacks = callbacks)\n","\n","\n","# 사용자 정의 콜백\n","on_train_begin() / on_train_end() / batch / epoch 등등\n","다양한 환경에서 데이터를 저장하고 관찰할 수 있다.\n","\n","class PrintValTrainRatioCallback(tf.keras.callbacks.Callback):\n","    def on_epoch_end(self, epoch, logs):\n","        ratio = logs[\"val_loss\"] / logs[\"loss\"]\n","        print(f\"Epoch={epoch}, val/train={ratio:.2f}\")\n","\"\"\""],"metadata":{"id":"4L8VwWjmCJ26"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","chat cpt가 순환 학습률을 구현한 사용자 정의 콜백\n","\n","from keras.callbacks import Callback\n","import keras.backend as K\n","\n","class CyclicLR(Callback):\n","    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular'):\n","        super().__init__()\n","\n","        self.base_lr = base_lr\n","        self.max_lr = max_lr\n","        self.step_size = step_size\n","        self.mode = mode\n","        self.clr_iterations = 0.\n","        self.trn_iterations = 0.\n","        self.history = {}\n","\n","    def clr(self):\n","        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n","        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n","        if self.mode == 'triangular':\n","            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x))\n","        # 기타 모드도 추가 가능\n","\n","    def on_train_begin(self, logs=None):\n","        logs = logs or {}\n","\n","        if self.clr_iterations == 0:\n","            K.set_value(self.model.optimizer.lr, self.base_lr)\n","        else:\n","            K.set_value(self.model.optimizer.lr, self.clr())\n","\n","    def on_batch_end(self, epoch, logs=None):\n","        logs = logs or {}\n","        self.trn_iterations += 1\n","        self.clr_iterations += 1\n","        K.set_value(self.model.optimizer.lr, self.clr())\n","\n","        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n","        self.history.setdefault('iterations', []).append(self.trn_iterations)\n","\n","        for k, v in logs.items():\n","            self.history.setdefault(k, []).append(v)\n","\n","# 모델 생성 및 컴파일\n","# model = ...\n","\n","# CyclicLR 콜백 사용 # lr을 0.06 ~ 0.6으로 해야할지도\n","clr = CyclicLR(base_lr=0.001, max_lr=0.6, step_size=2000., mode='triangular')\n","model.fit(X_train, Y_train, callbacks=[clr])\n","\n","\n","\"\"\""],"metadata":{"id":"9XfkqA6pq30Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","chat cpt가 순환 모멘텀을 구현한 사용자 정의(옵티마이저가 모멘텀 지원해야함)\n","from keras.callbacks import Callback\n","import keras.backend as K\n","\n","class CyclicLR(Callback):\n","    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular'):\n","        super().__init__()\n","\n","        self.base_lr = base_lr\n","        self.max_lr = max_lr\n","        self.step_size = step_size\n","        self.mode = mode\n","        self.clr_iterations = 0.\n","        self.trn_iterations = 0.\n","        self.history = {}\n","\n","    def clr(self):\n","        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n","        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n","        if self.mode == 'triangular':\n","            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x))\n","        # 기타 모드도 추가 가능\n","\n","    def on_train_begin(self, logs=None):\n","        logs = logs or {}\n","\n","        if self.clr_iterations == 0:\n","            K.set_value(self.model.optimizer.lr, self.base_lr)\n","        else:\n","            K.set_value(self.model.optimizer.lr, self.clr())\n","\n","    def on_batch_end(self, epoch, logs=None):\n","        logs = logs or {}\n","        self.trn_iterations += 1\n","        self.clr_iterations += 1\n","        K.set_value(self.model.optimizer.lr, self.clr())\n","\n","        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n","        self.history.setdefault('iterations', []).append(self.trn_iterations)\n","\n","        for k, v in logs.items():\n","            self.history.setdefault(k, []).append(v)\n","\n","# 모델 생성 및 컴파일\n","# model = ...\n","\n","# CyclicLR 콜백 사용\n","clr = CyclicLR(base_lr=0.9, max_lr=0.95, step_size=2000., mode='triangular')\n","model.fit(X_train, Y_train, callbacks=[clr])\n","\n","\"\"\""],"metadata":{"id":"OhPmbJHJq_ZF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","모델의 저장과 복원은 다음과 같이 한다.\n","import shutil\n","\n","#이미 존재하는 경우 삭제\n","shutil.rmtree(\"my_keras_model\", ignore_errors=True)\n","\n","model.save(\"my_keras_model\", save_format = \"tf\") #혹은 \"h5\"\n","model = tf.keras.models.load_model(\"my_keras_model\")\n","\n","model.save_weight(), load_weight()를 활용해 가중치만 저장하고 로드하는 것도 가능\n","\n","모델을 저장하면 my_keras_model이라는 폴더가 생성되고\n","\n","keras_metadata.pb에는 케라스에 필요한 추가 정보\n","variables 폴더에는 모든 파라미터값(가중치,편향,하이퍼 파라미터 등)\n","assets 폴더에는 데이터 샘플 특성 이름같은 추가 파일이 들어갈 수 있음(기본적으로는 비어있음)\n","\"\"\""],"metadata":{"id":"e-WKHUiGAqxN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 시퀀셜 API"],"metadata":{"id":"M959i_D81ksv"}},{"cell_type":"code","source":["from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","\n","housing = fetch_california_housing()\n","X_train_full, X_test, y_train_full, y_test = train_test_split(\n","    housing.data, housing.target, random_state=42)\n","X_train, X_valid, y_train, y_valid = train_test_split(\n","    X_train_full, y_train_full, random_state=42)"],"metadata":{"id":"GONNnMRG1n6L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","tf.random.set_seed(42)\n","norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])\n","model = tf.keras.Sequential([\n","    norm_layer,\n","    tf.keras.layers.Dense(50, activation=\"relu\"),\n","    tf.keras.layers.Dense(50, activation=\"relu\"),\n","    tf.keras.layers.Dense(50, activation=\"relu\"),\n","    tf.keras.layers.Dense(1)\n","])\n","optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n","model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n","#norm_layer는 sklearn의 StandardScaler와 같은데, 이처럼 훈련 전에 adapt해야한다.\n","norm_layer.adapt(X_train)\n","history = model.fit(X_train, y_train, epochs=20,\n","                    validation_data=(X_valid, y_valid))\n","mse_test, rmse_test = model.evaluate(X_test, y_test)\n","X_new = X_test[:3]\n","y_pred = model.predict(X_new)"],"metadata":{"id":"SSbUc9DB1p7G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 함수형 API (다중 입력, 다중 출력)"],"metadata":{"id":"7b7XxvtH2ZO3"}},{"cell_type":"code","source":["\"\"\"\n","함수형 API의 예시\n","\n","input = layers.Input(shape = X_trian.shape[1:])\n","normalized = layers.Normalization()(input)\n","hidden1 = layers.Dense(30, activation = 'relu')(normalized)\n","hideen2 = layers.Dense(30, activation = 'relu')(hidden1)\n","concat = layers.Concatenate()([normalized, hidden2])\n","output = layers.Dense(1)(concat)\n","\n","model = keras.Model(inputs = input, outputs = output)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n","model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n","normalization_layer.adapt(X_train) #잊지 말 것\n","history = model.fit(X_train, y_train, epochs=20,\n","                    validation_data=(X_valid, y_valid))\n","mse_test = model.evaluate(X_test, y_test)\n","y_pred = model.predict(X_new)\n","\"\"\""],"metadata":{"id":"IunBdqU72aAl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 다중 입력(특성 건너뛰기)\n","\n","# 모델 만들기\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import layers\n","\n","input_wide = layers.Input(shape=[5])  # 특성 0 ~ 4\n","norm_wide = layers.Normalization()(input_wide)\n","\n","input_deep = layers.Input(shape=[6])  # 특성 2 ~ 7\n","norm_deep = layers.Normalization()(input_deep)\n","hidden1 = layers.Dense(30, activation=\"relu\")(norm_deep)\n","hidden2 = layers.Dense(30, activation=\"relu\")(hidden1)\n","\n","concat = layers.concatenate([norm_wide, hidden2])\n","output = layers.Dense(1)(concat)\n","\n","model = keras.Model(inputs=[input_wide, input_deep], outputs=[output])\n","\n","# 컴파일, 정규화 adapt\n","optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n","model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n","\n","X_train_wide, X_train_deep = X_train[:, :5], X_train[:, 2:]\n","X_valid_wide, X_valid_deep = X_valid[:, :5], X_valid[:, 2:]\n","X_test_wide, X_test_deep = X_test[:, :5], X_test[:, 2:]\n","X_new_wide, X_new_deep = X_test_wide[:3], X_test_deep[:3]\n","\n","layers.Normalization().adapt(X_train_wide)\n","layers.Normalization().adapt(X_train_deep)\n","\n","# 모델 훈련 및 점수 확인 / 예측\n","history = model.fit((X_train_wide, X_train_deep), y_train, epochs=20,\n","                    validation_data=((X_valid_wide, X_valid_deep), y_valid))\n","mse_loss, rmse = model.evaluate((X_test_wide, X_test_deep), y_test)\n","y_pred = model.predict((X_new_wide, X_new_deep))"],"metadata":{"id":"emR2-0lr7cuC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","# 다중 입력과 다중 출력\n","1. 그림에 있는 물체를 분류하고 위치를 알아야 할 때\n","2. 동일한 데이터에서 독립적인 여러 작업을 수행할 때,\n","   예를 들어 같은 얼굴 이미지에서 한 출력은 표정을 출력하고,\n","   다른 출력은 안경 착용 유무를 출력하는 등.\n","\"\"\"\n","tf.keras.backend.clear_session()\n","tf.random.set_seed(42)\n","\n","input_wide = layers.Input(shape=[5])  # 특성 0 ~ 4\n","norm_wide = layers.Normalization()(input_wide)\n","\n","input_deep = layers.Input(shape=[6])  # 특성 2 ~ 7\n","norm_deep = layers.Normalization()(input_deep)\n","hidden1 = layers.Dense(30, activation=\"relu\")(norm_deep)\n","hidden2 = layers.Dense(30, activation=\"relu\")(hidden1)\n","\n","concat = layers.concatenate([norm_wide, hidden2])\n","output = layers.Dense(1)(concat)\n","aux_output = layers.Dense(1)(hidden2)\n","\n","model = keras.Model(inputs=[input_wide, input_deep],\n","                    outputs=[output, aux_output])\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n","\n","#loss를 각각 전달하고, 주 출력에 더 관심이 많다면(보조 출력은 보통 규제로\n","#사용되므로) 아래와 같이 가중치를 지정해주면 된다.\n","model.compile(loss=(\"mse\", \"mse\"), loss_weights=(0.9, 0.1), optimizer=optimizer,\n","              metrics=[\"RootMeanSquaredError\"])\n","\n","layers.Normalization().adapt(X_train_wide)\n","layers.Normalization().adapt(X_train_deep)\n","\n","#출력 2개가 y_train으로 같은데 이때 혼동을 방지하기 위해\n","#모델에 이름을 달고 {\"output\" : y_train, \"aux_output\" : y_train}처럼 쓸 수 있다.\n","history = model.fit(\n","    (X_train_wide, X_train_deep), # X\n","    (y_train, y_train), # Y\n","    epochs=20,\n","    validation_data= ((X_valid_wide, X_valid_deep),\n","                      (y_valid, y_valid))\n",")\n","\n","#이때 손실은 두 출력의 합과 각각 출력의 손실,metrics가 나온다\n","eval_results = model.evaluate((X_test_wide, X_test_deep), (y_test, y_test))\n","weighted_sum_of_losses, main_loss, aux_loss, main_rmse, aux_rmse = eval_results"],"metadata":{"id":"SjWoRSeW9iSA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 서브클래싱 API (연구용)"],"metadata":{"id":"MNfaHfw4_r29"}},{"cell_type":"code","source":["\"\"\"\n","서브클래싱 API는 저수준 텐서플로르 구현해 갖가지 for문 if문등을\n","구현할 수 있다는 장점이 있지만 동시에 문제를 추적하거나 모델을 해석하는것을\n","어렵게 하므로 왠만해서는 시퀀셜 API나 함수형 API를 사용하는 편이 낫다.\n","\"\"\"\n","class WideAndDeepModel(tf.keras.Model):\n","    def __init__(self, units=30, activation=\"relu\", **kwargs):\n","        super().__init__(**kwargs)  # 모델 이름을 지정하는 데 필요합니다\n","        self.norm_layer_wide = tf.keras.layers.Normalization()\n","        self.norm_layer_deep = tf.keras.layers.Normalization()\n","        self.hidden1 = tf.keras.layers.Dense(units, activation=activation)\n","        self.hidden2 = tf.keras.layers.Dense(units, activation=activation)\n","        self.main_output = tf.keras.layers.Dense(1)\n","        self.aux_output = tf.keras.layers.Dense(1)\n","\n","    def call(self, inputs):\n","        input_wide, input_deep = inputs\n","        norm_wide = self.norm_layer_wide(input_wide)\n","        norm_deep = self.norm_layer_deep(input_deep)\n","        hidden1 = self.hidden1(norm_deep)\n","        hidden2 = self.hidden2(hidden1)\n","        concat = tf.keras.layers.concatenate([norm_wide, hidden2])\n","        output = self.main_output(concat)\n","        aux_output = self.aux_output(hidden2)\n","        return output, aux_output\n","\n","tf.random.set_seed(42)  # 추가 코드 - 재현성을 위한 것\n","model = WideAndDeepModel(30, activation=\"relu\", name=\"my_cool_model\")\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n","model.compile(loss=\"mse\", loss_weights=[0.9, 0.1], optimizer=optimizer,\n","              metrics=[\"RootMeanSquaredError\"])\n","model.norm_layer_wide.adapt(X_train_wide)\n","model.norm_layer_deep.adapt(X_train_deep)\n","history = model.fit(\n","    (X_train_wide, X_train_deep), (y_train, y_train), epochs=10,\n","    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid)))\n","eval_results = model.evaluate((X_test_wide, X_test_deep), (y_test, y_test))\n","weighted_sum_of_losses, main_loss, aux_loss, main_rmse, aux_rmse = eval_results\n","y_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))"],"metadata":{"id":"iDpN3Xqg_uYJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 텐서보드 (시각화)"],"metadata":{"id":"8xakJ1RHLsP0"}},{"cell_type":"code","source":["#코랩에서 텐서보드 설치\n","%pip install -q -U tensorboard-plugin-profile"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W-ek1srtLtsl","executionInfo":{"status":"ok","timestamp":1699769966401,"user_tz":-540,"elapsed":6874,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"b306ec55-4c87-4baf-c89b-01fab2a3cb92"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/5.6 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/5.6 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["#기본 설정\n","from pathlib import Path\n","from time import strftime\n","import tensorflow as tf\n","\n","#파일 이름 설정\n","def get_run_logdir(root_logdir = \"my_logs\"):\n","    return Path(root_logdir) / strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n","\n","run_logdir = get_run_logdir()\n","\n","#텐서보드 콜백. 에포크동안 100~200배치의 신경망을 프로파일링 한다.\n","tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir,\n","                                                profile_batch = (100, 200))\n","# 콜백에 담아 훈련한다.\n","# history = model.fit([...], callbacks = tensorboard_cb)\n","\n","#텐서보드 열기\n","%load_ext tensorboard\n","%tensorboard --logdir=./my_logs #콜백 파일 접근\n","\n","\"\"\"\n","히스토그램, 이미지, 텍스트를 시각화할 수도 있고, 오디오를 들을 수도 있다\n","test_logdir = get_run_logdir()\n","writer = tf.summary.create_file_writer(str(test_logdir))\n","with writer.as_default():\n","    for step in range(1, 1000 + 1):\n","        tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step=step)\n","\n","        data = (np.random.randn(100) + 2) * step / 100  # gets larger\n","        tf.summary.histogram(\"my_hist\", data, buckets=50, step=step)\n","\n","        images = np.random.rand(2, 32, 32, 3) * step / 1000  # gets brighter\n","        tf.summary.image(\"my_images\", images, step=step)\n","\n","        texts = [\"The step is \" + str(step), \"Its square is \" + str(step ** 2)]\n","        tf.summary.text(\"my_text\", texts, step=step)\n","\n","        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)\n","        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n","        tf.summary.audio(\"my_audio\", audio, sample_rate=48000, step=step)\n","\"\"\""],"metadata":{"id":"2zvYaTtYMFvh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 케라스 튜너"],"metadata":{"id":"ftcrxcXpN44t"}},{"cell_type":"code","source":["#코랩에서 케라스 튜너 설치\n","%pip install -q -U keras-tuner"],"metadata":{"id":"PakP451AN6bk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","이렇게 쓰는 것도 가능하다. 다만 아래 코드처럼 층 개수는 안정함.\n","def build_model(hp):\n","    model = keras.Sequential()\n","    model.add(layers.Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'))\n","    model.add(layers.Dense(1, activation='sigmoid'))\n","    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n","                  loss='binary_crossentropy',\n","                  metrics=['accuracy'])\n","    return model\n","\"\"\"\n","\n","\"\"\"\n","build_model 예시\n","\n","def build_model(hp):\n","    model = keras.Sequential()\n","    model.add(keras.layers.InputLayer(input_shape=(input_shape,)))\n","\n","    # 은닉 레이어 수와 각 레이어의 유닛 수를 하이퍼파라미터로 설정\n","    for i in range(hp.Int('num_layers', 1, 5)):\n","        model.add(keras.layers.Dense(units=hp.Int('units_' + str(i),\n","                                     min_value=32, max_value=512, step=32),\n","                                     activation='relu'))\n","    model.add(keras.layers.Dense(1, activation='sigmoid'))\n","    model.compile(optimizer= hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop']) # 여러가지 옵티마이저 고르기\n","                  loss='binary_crossentropy',\n","                  metrics=['accuracy'])\n","    return model\n","\n","    또는 학습률을 조정하기\n","    model.compile(optimizer=keras.optimizers.Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n","                  loss='binary_crossentropy',\n","                  metrics=['accuracy'])\n","\n","\"\"\"\n","\n","\n","\"\"\"\n","학습률을 동적으로 조정하는 하이퍼파라미터 스케줄링도 있다. arxiv 1711.09846\n","학습률, 여러 가지 드랍아웃 rate를 조정할  수 있다.\n","GAN에서는 생성자와 판별자의 학습률을 조정한다던지..\n","\n","ExponentialDecay: 매 스텝마다 학습률이 decay_rate만큼 곱해지며,\n","                  일반적으로 decay_steps 마다 업데이트됩니다.\n","데이터 샘플의 개수가 60000개이고, 미니배치 사이즈가 32이고 decay_steps가 10000일때\n","한 에포크당 60000 / 32 = 1875개의 배치가 처리되므로\n","10000 / 1875 = 5.53에포크마다 학습률이 조정된다.\n","\n","\n","initial_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n","\n","lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n","    initial_learning_rate=initial_learning_rate,\n","    decay_steps=10000,\n","    decay_rate=0.9)\n","\n","model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","\"\"\""],"metadata":{"id":"qzSoy-x7T0Dw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import keras_tuner as kt\n","\n","fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n","(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n","X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n","X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n","\n","def build_model(hp): #hp = 하이퍼 파라미터\n","# 이렇게 n_hidden 파라미터를 만드는데, 이 파라미터가 내부에 없으면\n","# 자동으로 추가한다.\n","\n","    n_hidden = hp.Int(\"n_hidden\", min_value=0, max_value=8, default=2) # 층의 개수\n","    n_neurons = hp.Int(\"n_neurons\", min_value=16, max_value=256)  # 유닛의 개수\n","    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, # 학습률 조정\n","                             sampling=\"log\")\n","    optimizer = hp.Choice(\"optimizer\", values=[\"sgd\", \"adam\"]) #옵티마이저 고르기(learning_rate를 적용하기 위해 아래처럼 구성함)\n","    if optimizer == \"sgd\":\n","        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n","    else:\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","\n","    model = tf.keras.Sequential()\n","    model.add(tf.keras.layers.Flatten())\n","    for _ in range(n_hidden):\n","        model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\"))\n","    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n","    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n","                  metrics=[\"accuracy\"])\n","    return model\n","\n","random_search_tuner = kt.RandomSearch(\n","    build_model, objective=\"val_accuracy\", max_trials=5, overwrite=True,\n","    directory=\"my_fashion_mnist\", project_name=\"my_rnd_search\", seed=42)\n","random_search_tuner.search(X_train, y_train, epochs=10,\n","                           validation_data=(X_valid, y_valid))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gQ5MmwnMON0j","executionInfo":{"status":"ok","timestamp":1699771195980,"user_tz":-540,"elapsed":547739,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"440485c3-b6cc-429b-d0dd-078927413190"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Trial 5 Complete [00h 02m 23s]\n","val_accuracy: 0.8416000008583069\n","\n","Best val_accuracy So Far: 0.8641999959945679\n","Total elapsed time: 00h 09m 07s\n"]}]},{"cell_type":"code","source":["top3_models = random_search_tuner.get_best_models(num_models=3)\n","best_model = top3_models[0]\n","\n","top3_params = random_search_tuner.get_best_hyperparameters(num_trials=3)\n","top3_params[0].values  # best hyperparameter values"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5x0qENMHPx08","executionInfo":{"status":"ok","timestamp":1699771274697,"user_tz":-540,"elapsed":1669,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"cf7efa79-b78d-4e84-97e5-76983d5bbb41"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'n_hidden': 8,\n"," 'n_neurons': 37,\n"," 'learning_rate': 0.008547485565344062,\n"," 'optimizer': 'sgd'}"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["best_trial = random_search_tuner.oracle.get_best_trials(num_trials=1)[0]\n","best_trial.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NHCyI4X7QAEH","executionInfo":{"status":"ok","timestamp":1699771278383,"user_tz":-540,"elapsed":268,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"79f625cd-934b-4366-c8b3-d0d145a573a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Trial 3 summary\n","Hyperparameters:\n","n_hidden: 8\n","n_neurons: 37\n","learning_rate: 0.008547485565344062\n","optimizer: sgd\n","Score: 0.8641999959945679\n"]}]},{"cell_type":"code","source":["best_trial.metrics.get_last_value(\"val_accuracy\") #가장 좋은 점수 확인"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zdT7GcDSQDNv","executionInfo":{"status":"ok","timestamp":1699771282795,"user_tz":-540,"elapsed":1,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"d7149234-a252-4337-a168-1dc55f5cca83"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8641999959945679"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["#마지막 훈련\n","#best_model = random_search_tuner.get_best_models()\n","best_model.fit(X_train_full, y_train_full, epochs=10)\n","test_loss, test_accuracy = best_model.evaluate(X_test, y_test)"],"metadata":{"id":"T4iWVfCKQE8b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","상황에 따라 데이터 전처리 하이퍼파라미터 또는\n","배치 크기와 같은 model.fit() 매개변수를 미세 조정해야 할 수 있는데\n","이때는 사용자 정의 모델을 만들어서 튜너에 전달해야 한다.\n","\"\"\"\n","# 정규화를 추가한 모델\n","class MyClassificationHyperModel(kt.HyperModel):\n","    def build(self, hp):\n","        return build_model(hp) #위에서 만든 모델\n","\n","    def fit(self, hp, model, X, y, **kwargs): #fit 메서드만 새로 만듬\n","        if hp.Boolean(\"normalize\"):\n","            norm_layer = tf.keras.layers.Normalization()\n","            X = norm_layer(X)\n","        return model.fit(X, y, **kwargs)\n","\n","\"\"\"\n","하이퍼밴드 튜너\n","몇 번의 에포크동안 여러 모델을 훈련한 다음, 상위 1 / factor(3)의 모델만 남긴다.\n","이 과정을 단일 모델이 남을때까지 반복한다.\n","사용 방법은 위의 랜덤 서칭과 거의 동일함.\n","\n","max_epochs : 각 하이퍼파라미터 설정을 위한 최대 에포크 수\n","factor : factor가 3이면 각 반복마다 모델 수를 1/3로 줄임.\n","min_epochs(선택적) : 각 하이퍼파라미터 설정의 최소 에포크 수\n","objective : 최적화하고자 하는 목표 지표 ('val_accuracy', 'val_loss' 등)\n","hyperband_iterations(선택적): 전체 hyperband 알고리즘을 몇 번 실행할지\n","diretory = 파일 경로\n","project_name = 폴더 이름\n","\n","\"\"\"\n","hyperband_tuner = kt.Hyperband(\n","    MyClassificationHyperModel(), objective=\"val_accuracy\", seed=42,\n","    max_epochs=10, factor=3, hyperband_iterations=2,\n","    overwrite=True, directory=\"my_fashion_mnist\", project_name=\"hyperband\")"],"metadata":{"id":"DRJ6XmDuQgYl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["root_logdir = Path(hyperband_tuner.project_dir) / \"tensorboard\"\n","tensorboard_cb = tf.keras.callbacks.TensorBoard(root_logdir)\n","early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=2)\n","hyperband_tuner.search(X_train, y_train, epochs=10,\n","                       validation_data=(X_valid, y_valid),\n","                       callbacks=[early_stopping_cb, tensorboard_cb])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rZZi9_dnTGDq","executionInfo":{"status":"ok","timestamp":1699774344997,"user_tz":-540,"elapsed":2471488,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"fa43fa13-3771-4c09-d64f-eb027f51cb45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Trial 60 Complete [00h 01m 27s]\n","val_accuracy: 0.8565999865531921\n","\n","Best val_accuracy So Far: 0.8784000277519226\n","Total elapsed time: 00h 41m 12s\n"]}]},{"cell_type":"code","source":["best_trial = hyperband_tuner.oracle.get_best_trials(num_trials=1)[0]\n","best_trial.summary()"],"metadata":{"id":"ekzrGW4dtaoM","executionInfo":{"status":"ok","timestamp":1699778770826,"user_tz":-540,"elapsed":2,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"b3b94ddc-c86c-464a-82e8-18fe3293687a","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Trial 0054 summary\n","Hyperparameters:\n","n_hidden: 7\n","n_neurons: 247\n","learning_rate: 0.0003987720809096887\n","optimizer: adam\n","normalize: True\n","tuner/epochs: 10\n","tuner/initial_epoch: 4\n","tuner/bracket: 1\n","tuner/round: 1\n","tuner/trial_id: 0049\n","Score: 0.8784000277519226\n"]}]},{"cell_type":"markdown","source":["정규화를 추가한 하이퍼밴드 튜너\\\n","학습률 0.00039877\\\n","뉴런 247.00\\\n","정규화 true\\\n","층 7.0000\\\n","옵티마이저 adam\\\n","\n","검증 정확도 : 0.8784000277519226\n","\n","best_trial = hyperband_tuner.oracle.get_best_trials(num_trials=1)[0]\n","best_trial.summary()"],"metadata":{"id":"iWNE2KqatSw2"}},{"cell_type":"code","source":["\"\"\"\n","하이퍼밴드나, 랜덤검색은 결국 하이퍼파라미터 공간을 랜덤으로 탐색한다는 점 때문에\n","빠르지만 듬성듬성하다는 단점이 있다. 이것을 보완하기 위한것으로\n","베이지안 옵티마이저가 있는데,\n","alpha는 성능 측정에서 예상되는 잡음 수준을\n","beat는 알고리즘이 얼마나 공간을 탐색할지를 지정한다. (나머지는\n","\"\"\"\n","bayesian_opt_tuner = kt.BayesianOptimization(\n","    MyClassificationHyperModel(), objective=\"val_accuracy\", seed=42,\n","    max_trials=10, alpha=1e-4, beta=2.6,\n","    overwrite=True, directory=\"my_fashion_mnist\", project_name=\"bayesian_opt\")\n","bayesian_opt_tuner.search(X_train, y_train, epochs=10,\n","                          validation_data=(X_valid, y_valid),\n","                          callbacks=[early_stopping_cb])"],"metadata":{"id":"EETnvNFSZZgg"},"execution_count":null,"outputs":[]}]}