{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMBBtMKCKV7h8ZjI8+rDxC/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from sklearn.datasets import make_moons\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","\n","X, y = make_moons(n_samples = 500, noise = 0.3, random_state = 42)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n","\n","\"\"\"\n","VotingClassifier는 여러 개의 모델을 estimators에 넣고 각자 모델을 훈련하여\n","각각의 모델이 각자의 예측을 반환하고 그 예측의 과반수를 선택하는 분류기다.\n","앙상블 모델은 앙상블을 이루는 모델이 독립적일수록 매우 다른 종류의 오차를\n","반환하므로 일반적으로 성능이 향상된다.\n","\"\"\"\n","voting_clf = VotingClassifier(\n","    estimators = [\n","        ('lr', LogisticRegression(random_state = 42)),\n","        ('rf', RandomForestClassifier(random_state = 42)),\n","        ('svc', SVC(random_state = 42))\n","    ]\n",")\n","\n","voting_clf.fit(X_train, y_train)"],"metadata":{"id":"rgrqfoY7g_zJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["voting_clf.score(X_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KAWYV11XiP26","executionInfo":{"status":"ok","timestamp":1699591587648,"user_tz":-540,"elapsed":2,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"8c26b4b4-3401-4c5e-9250-63a08af0e810"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.912"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["\"\"\"모든 분류기가 다중 분류를 지원하면 (predict_proba 메서드가 있으면)\n","간접 투표 방식을 시행할 수 있다. 이는 위의 단순한 방법보다 더욱 성능이 좋다.\n","\"\"\"\n","\n","voting_clf.voting = 'soft'\n","voting_clf.named_estimators[\"svc\"].probability = True # 이렇게 직접 수정도 가능\n","voting_clf.fit(X_train, y_train)\n","voting_clf.score(X_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PdRR7morjeYP","executionInfo":{"status":"ok","timestamp":1699591720671,"user_tz":-540,"elapsed":422,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"3e433090-ecc8-41f9-da49-dcef8882c2d9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.92"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["\"\"\"\n","Bagging (Bootstrap aggregationg) : 훈련 세트에서 중복을 허용하여 샘플링(뽑고 다시 넣고 뽑고)\n","Pasting : 중복을 허용하지 않음\n","\n","배깅을 사용하면 하나의 알고리즘을 사용한 여러개의 분류기를 각기 다르게 훈련하여\n","분류의 경우에는(최빈 예측값) 회귀의 경우에는 평균값을 계산해 더욱 좋은 결과를 얻을 수 있다.\n","배깅은 편향을 손해보고 분산을 줄이는 방식이로 전체적으로 더 일반화된 모델을 만든다.\n","\"\"\"\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# 결정트리 500개를 배깅 앙상블로 만드는 것.\n","# bootstrap = False로 지정하면 페이스팅으로 훈련함.\n","# oob_score = True로 지정하면 Out of Bag 샘플을 가지고 검증세트로 활용\n","bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators = 500,\n","                            max_samples = 100, n_jobs = -1, random_state = 42,\n","                            bootstrap = True, oob_score = True)\n","bag_clf.fit(X_train, y_train)\n","print(f\"예측 점수 :{bag_clf.score(X_test, y_test)}\")\n","print(f\"OOB 점수 : {bag_clf.oob_score_.round(3)}\")\n","\n","y_pred = bag_clf.predict(X_test)\n","print(f\"테스트 세트 정확도: {accuracy_score(y_test, y_pred)}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hUVBob0HniPQ","executionInfo":{"status":"ok","timestamp":1699593317813,"user_tz":-540,"elapsed":3033,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"202ffa32-5d07-4992-8345-c254a7caccfc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["예측 점수 :0.904\n","OOB 점수 : 0.925\n","테스트 세트 정확도: 0.904\n"]}]},{"cell_type":"code","source":["#랜덤포레스트 모델은 기본적으로 샘플과 특성에 배깅(또는 페이스팅)을 적용한다.\n","from sklearn.ensemble import RandomForestClassifier\n","\n","#샘플 개수를 지정하지 않으면 전체를 배깅함\n","#특성의 경우는 sqrt(n)개를 골라서 무작위성을 주입함.\n","rnd_clf = RandomForestClassifier(n_estimators = 500, max_leaf_nodes = 16,\n","                                 n_jobs = -1, random_state = 42)\n","rnd_clf.fit(X_train, y_train)\n","\n","#랜덤 포레스트는 각 특성의 중요도를 반환한다.\n","rnd_clf.feature_importances_\n","\n","\"\"\"\n","일반적인 랜덤 포레스트보다 더 극단적으로 랜덤(Extremely Random)하고\n","더욱 빠른 트리 알고리즘을 Extra Tree라고 한다.\n","ensemble.ExtraTreesClassifier()\n","기본적으로 모든 샘플을 사용한다. bootstrap = False\n","\"\"\"\n"],"metadata":{"id":"5EwvcRMVp2gr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Boosting은 약한 학습기를 연결해서 강한 학습기를 만드는 앙상블 기법으로\n","앞의 모델의 가중치(AdaBoost)나 잔여 오차(GBRT)를 기반으로 다음 모델을\n","지속적으로 훈련시켜 더욱 강력한 모델을 만드는 방법을 말한다.\n","\n","n_estimator = 연결 시킬 모델의 개수\n","learning_rate = 각 모델(트리)의 기여도, 낮게 설정할수록 많은 트리가 필요하지만\n","                일반적으로 예측의 성능은 향상된다.\n","\"\"\"\n","from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier\n","\n","ada_clf = AdaBoostClassifier(\n","    DecisionTreeClassifier(max_depth = 1), n_estimators = 30,\n","    learning_rate = 0.5, random_state = 42)\n","\n","gbrt = GradientBoostingClassifier(max_depth = 2, n_estimaotr = 5,\n","                                  learning_rate = 1.0, random_state = 42)\n","\"\"\"\n","gbrt의 n_iter_no_change = 10처럼 설정하면 early_stopping을 구현할 수 있다.\n","n_iter_no_change를 설정하면 fit메서드가 자동으로 훈련세트를 작은 훈련세트와\n","검증 세트로 분할하며. 이 비율은 validation_fraction = 0.1(10%)처럼 지정할 수 있다.\n","또한 tol = 1e-4(0.0001)은 무시할 수 있는 최대 성능 향상값을 결정한다.\n","\n","subsample = 0.25는 각 트리가 훈련할 때 사용할 훈련 샘플의 비율이다.\n","각 트리 훈련이 시작할때마다 25%씩 랜덤으로 골라서 훈련하며\n","이는 편향이 높아지는 대신 분산이 낮아지고, 훈련 속도 또한 빨라지는 효과를 가진다.\n","(Stochastic gradient boosting)\n","\"\"\"\n"],"metadata":{"id":"7J7PFtNE1VBt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","추가로 sample이 10000개 이상이 되면 HistGradientBoosting이 더욱 효과적인데\n","계산복잡도가 O(b*m) b = 구간의 개수(max = 255), m = sample의 개수 이므로\n","엄청나게 빠른 계산을 구현한다. 다만 구간 분할 자체가 규제처럼 작동해\n","정밀도 손실을 유발하므로, 과대적합이나 과소적합의 원인이 될 수 있다.\n","\n","특징:\n","1. 전처리 필요 없음(트리가 그렇듯)\n","2. 특성을 신경 쓸 필요 없음(특성 정렬 불필요)\n","3. 훈련 속도가 매우 빠름(O(b*m))\n","4. 범주형 특성과 누락된 값 지원\n","\n","일반 그레이디언트 부스팅과의 차이점:\n","1. subsample 지원하지 않음\n","2. n_estimator가 max_iter로 바뀜\n","3. max_leaf_nodes, min_samples_leaf, max_depth, max_bins정도밖에 설정 불가능\n","\n","\n","아래는 예시\n","1. 범주형 특성을 0~ max_bins사이의 정수로 표현하기 위해 OrdinalEncoder 사용\n","2. 범주형 열의 인덱스로 categorical_features = [n] 을 설정해야함.\n","\n","from sklearn.pipeline import make_pipeline\n","from sklearn.compose import make_column_transformer\n","from sklearn.ensemble import HistGradientBoostingRegressor\n","from sklearn.preprocessing import OrdinalEncoder\n","\n","hgb_reg = make_pipeline(\n","    make_column_transformer((OrdinalEncoder(), [\"ocean_proximity\"]),\n","                            remainder=\"passthrough\"),\n","    HistGradientBoostingRegressor(categorical_features=[0], random_state=42)\n",")\n","hgb_reg.fit(housing, housing_labels)\n","\"\"\""],"metadata":{"id":"r-jruQyu4iAf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Stacking (Stacked generalization)은 각각의 모델의 예측을 하나로 합친\n","블렌딩 훈련 세트를 마지막 예측기(Blender or Meta learner)의 입력으로 사용해\n","마지막 예측기를 훈련시켜 최종 예측을 만드는 앙상블 기법이다.\n","\"\"\"\n","\n","from sklearn.ensemble import StackingClassifier\n","\n","stacking_clf = StackingClassifier(\n","    estimators=[\n","        ('lr', LogisticRegression(random_state=42)),\n","        ('rf', RandomForestClassifier(random_state=42)),\n","        ('svc', SVC(probability=True, random_state=42))\n","    ],\n","    final_estimator=RandomForestClassifier(random_state=43),\n","    cv=5  # 교차 검증 폴드 수\n",")\n","stacking_clf.fit(X_train, y_train)\n","stacking_clf.score(X_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PdOVMAeP5hHW","executionInfo":{"status":"ok","timestamp":1699597850034,"user_tz":-540,"elapsed":3720,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"e106b5fc-442c-4473-d052-a132f363c95c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.928"]},"metadata":{},"execution_count":40}]}]}