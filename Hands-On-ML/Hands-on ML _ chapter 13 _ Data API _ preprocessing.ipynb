{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Yp_KVtRQSVCo"],"authorship_tag":"ABX9TyPw13Fkv8tH45B1SIVKjPxE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **데이터 API(기초, CSV 파일 기준)**\n"],"metadata":{"id":"YEzkCwL87ndo"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","X = tf.range(10)  # 데이터 텐서(그냥 예시)\n","dataset = tf.data.Dataset.from_tensor_slices(X) #데이터셋 만들어짐\n","dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XD3EuD497wMr","executionInfo":{"status":"ok","timestamp":1699933537654,"user_tz":-540,"elapsed":5300,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"9ec38e07-305e-4804-a5de-646c82248dd8"},"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["#순회하여 확인 가능\n","for item in dataset:\n","    print(item)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qIy1isMl7xrF","executionInfo":{"status":"ok","timestamp":1699933549644,"user_tz":-540,"elapsed":284,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"3754910c-19f4-4c09-d58a-c7db64ab2301"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(0, shape=(), dtype=int32)\n","tf.Tensor(1, shape=(), dtype=int32)\n","tf.Tensor(2, shape=(), dtype=int32)\n","tf.Tensor(3, shape=(), dtype=int32)\n","tf.Tensor(4, shape=(), dtype=int32)\n","tf.Tensor(5, shape=(), dtype=int32)\n","tf.Tensor(6, shape=(), dtype=int32)\n","tf.Tensor(7, shape=(), dtype=int32)\n","tf.Tensor(8, shape=(), dtype=int32)\n","tf.Tensor(9, shape=(), dtype=int32)\n"]}]},{"cell_type":"code","source":["# 이렇게 만들 수도 있다.\n","X_nested = {\"a\": ([1, 2, 3], [4, 5, 6]), \"b\": [7, 8, 9]}\n","dataset = tf.data.Dataset.from_tensor_slices(X_nested)\n","for item in dataset:\n","    print(item)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YXnuDnID7_Wq","executionInfo":{"status":"ok","timestamp":1699933635186,"user_tz":-540,"elapsed":2,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"4df62d29-235b-4b05-eccd-5c0aa8249d5f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=int32, numpy=4>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=7>}\n","{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(), dtype=int32, numpy=5>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=8>}\n","{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=3>, <tf.Tensor: shape=(), dtype=int32, numpy=6>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=9>}\n"]}]},{"cell_type":"code","source":["#연쇄 변환\n","dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n","#0~9까지 3번 순회하고 길이는 7로 자른다.\n","dataset = dataset.repeat(3).batch(7) #batch(7, drop_remainder = True)로 호출하면 밑에 2개는 자름\n","for item in dataset:\n","    print(item)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D1Rmdkpz8pIN","executionInfo":{"status":"ok","timestamp":1699933887842,"user_tz":-540,"elapsed":2,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"1fc2af64-dc6e-473d-ac91-41791c1d7c51"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n","tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n","tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n","tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n","tf.Tensor([8 9], shape=(2,), dtype=int32)\n"]}]},{"cell_type":"code","source":["dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n","dataset = dataset.map(lambda x: x ** 2)  # x는 하나의 배치이다.\n","for item in dataset:\n","    print(item)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8SL0ZiZw9T3-","executionInfo":{"status":"ok","timestamp":1699934150113,"user_tz":-540,"elapsed":303,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"e762d71b-d40b-4e3b-f657-531fd2fa44bc"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(0, shape=(), dtype=int32)\n","tf.Tensor(1, shape=(), dtype=int32)\n","tf.Tensor(4, shape=(), dtype=int32)\n","tf.Tensor(9, shape=(), dtype=int32)\n","tf.Tensor(16, shape=(), dtype=int32)\n","tf.Tensor(25, shape=(), dtype=int32)\n","tf.Tensor(36, shape=(), dtype=int32)\n","tf.Tensor(49, shape=(), dtype=int32)\n","tf.Tensor(64, shape=(), dtype=int32)\n","tf.Tensor(81, shape=(), dtype=int32)\n"]}]},{"cell_type":"code","source":["dataset = dataset.filter(lambda x: tf.reduce_sum(x) > 20) #이렇게 필터링도 가능\n","for item in dataset:\n","    print(item)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J2iXSiwQ98Pa","executionInfo":{"status":"ok","timestamp":1699934159502,"user_tz":-540,"elapsed":1,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"d5c07dad-4a53-4a69-9b19-3a730008a786"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(25, shape=(), dtype=int32)\n","tf.Tensor(36, shape=(), dtype=int32)\n","tf.Tensor(49, shape=(), dtype=int32)\n","tf.Tensor(64, shape=(), dtype=int32)\n","tf.Tensor(81, shape=(), dtype=int32)\n"]}]},{"cell_type":"code","source":["for item in dataset.take(3): # 위에서 몇개만 짜를수도 있다.\n","    print(item)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yciRX-PL9_yW","executionInfo":{"status":"ok","timestamp":1699934173317,"user_tz":-540,"elapsed":299,"user":{"displayName":"김태식","userId":"11264777984025639378"}},"outputId":"59e4603d-3050-40c6-f024-70de0025b1a5"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(25, shape=(), dtype=int32)\n","tf.Tensor(36, shape=(), dtype=int32)\n","tf.Tensor(49, shape=(), dtype=int32)\n"]}]},{"cell_type":"code","source":["\"\"\"\n","데이터 Shuffle\n","예를 들어 왼쪽에 카드 Deck 뭉치가 있을때 카드 뭉치에서 buffer_size개만큼 카드를 뽑고\n","오른쪽에 하나 내려놓고, 다시 왼쪽에서 하나 뽑고..를 반복해서 오른쪽에 카드 Deck을 만드는 것이다.\n","이것은 buffer_size가 작을수록 완전한 Shuffle이 이루어지지 않는다는 것을 시사한다.\n","\n","이를 해결하는 방법\n","1. 버퍼를 늘린다(데이터셋이 너무 크면 불가능)\n","2. 원본 샘플을 섞는다(데이터셋이 커도 가능.)\n","\"\"\"\n","dataset = tf.data.Dataset.range(10).repeat(2) #[0,1,2,3,4...,0,1,2,3,4...]\n","\n","dataset = dataset.shuffle(buffer_size=4, seed=42).batch(7)\n","for item in dataset:\n","    print(item)\n","\n","\"\"\"\n","dataset = tf.data.Dataset.range(10).repeat(2) #[0,1,2,3,4...,0,1,2,3,4...]\n","dataset = dataset.shuffle(buffer_size=4, reshuffle_each_iteration = False,\n","                          seed=42).repeat(3).batch(7)\n","for item in dataset:\n","    print(item)\n","과 같이 쓰면 shuffle을 x3번 한 배열을 생성하는데, 그 배열들 순서가 다 똑같이 나옴\n","\"\"\"\n"],"metadata":{"id":"tb3dIIXT-3uT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**그렇다면 원본 샘플은 어떻게 섞을까?**\n","\n","1. 먼저 데이터 셋을 불러오고\n","\n","```\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from pathlib import Path\n","\n","housing = fetch_california_housing()\n","X_train_full, X_test, y_train_full, y_test = train_test_split(\n","    housing.data, housing.target.reshape(-1, 1), random_state=42)\n","X_train, X_valid, y_train, y_valid = train_test_split(\n","    X_train_full, y_train_full, random_state=42)\n","```\n","2. 데이터셋을 분할한다(아래는 20개로)\n","\n","```\n","def save_to_csv_files(data, name_prefix, header=None, n_parts=10):\n","    housing_dir = Path() / \"datasets\" / \"housing\"\n","    housing_dir.mkdir(parents=True, exist_ok=True)\n","    filename_format = \"my_{}_{:02d}.csv\"\n","\n","    filepaths = []\n","    m = len(data)\n","    chunks = np.array_split(np.arange(m), n_parts)\n","    for file_idx, row_indices in enumerate(chunks):\n","        part_csv = housing_dir / filename_format.format(name_prefix, file_idx)\n","        filepaths.append(str(part_csv))\n","        with open(part_csv, \"w\") as f:\n","            if header is not None:\n","                f.write(header)\n","                f.write(\"\\n\")\n","            for row_idx in row_indices:\n","                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n","                f.write(\"\\n\")\n","    return filepaths\n","\n","train_data = np.c_[X_train, y_train]\n","valid_data = np.c_[X_valid, y_valid]\n","test_data = np.c_[X_test, y_test]\n","header_cols = housing.feature_names + [\"MedianHouseValue\"]\n","header = \",\".join(header_cols)\n","\n","train_filepaths = save_to_csv_files(train_data, \"train\", header, n_parts=20)\n","valid_filepaths = save_to_csv_files(valid_data, \"valid\", header, n_parts=10)\n","test_filepaths = save_to_csv_files(test_data, \"test\", header, n_parts=10)\n","```\n","\n","3. 그러면 파일이 20개로 분할된 것을 볼 수 있고\n","\n","```\n","train_filepaths\n","->['datasets/housing/my_train_00.csv',\n","'datasets/housing/my_train_01.csv',\n","'datasets/housing/my_train_02.csv',\n","'datasets/housing/my_train_03.csv',\n","'datasets/housing/my_train_04.csv',\n","'datasets/housing/my_train_05.csv',\n","'datasets/housing/my_train_06.csv',\n","                [...]\n","'datasets/housing/my_train_15.csv',\n","'datasets/housing/my_train_16.csv',\n","'datasets/housing/my_train_17.csv',\n","'datasets/housing/my_train_18.csv',\n","'datasets/housing/my_train_19.csv']\n","```\n","\n","4. 파일 경로를 섞은 다음...\n","\n","```\n","filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\n","for filepath in filepath_dataset:\n","    print(filepath)\n"," ->\n","tf.Tensor(b'datasets/housing/my_train_05.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_16.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_01.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_17.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_00.csv', shape=(), dtype=string)\n","                                 [...]\n","tf.Tensor(b'datasets/housing/my_train_18.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_04.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_06.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_03.csv', shape=(), dtype=string)\n","tf.Tensor(b'datasets/housing/my_train_08.csv', shape=(), dtype=string)\n","```\n","\n","5.마지막으로 순서대로 5개를 불러올 수 있다.   \n","파일 5개를 불러오고, 순서대로 한 줄씩 읽고, 다 읽으면 5개 불러오는 식이다.   \n","* interleave는 (num_parallel_calls = n) 매개변수로 병렬화를 지원한다.   \n","* TextLineDataset에 파일 경로 리스트를 전달할수도 있지만 파일을 섞거나   \n","헤더를 (skip(1))을 건너뛰지 못한다.\n","\n","```\n","n_readers = 5\n","dataset = filepath_dataset.interleave(\n","    lambda filepath: tf.data.TextLineDataset(filepath).skip(1), #첫번째 줄은 열 이름\n","    cycle_length=n_readers)\n","\n"," for line in dataset.take(5):\n","    print(line)\n"," ->tf.Tensor(b'4.5909,16.0,5.475877192982456,1.0964912280701755,1357.0,2.9758771929824563,33.63,-117.71,2.418', shape=(), dtype=string)\n","tf.Tensor(b'2.4792,24.0,3.4547038327526134,1.1341463414634145,2251.0,3.921602787456446,34.18,-118.38,2.0', shape=(), dtype=string)\n","tf.Tensor(b'4.2708,45.0,5.121387283236994,0.953757225433526,492.0,2.8439306358381504,37.48,-122.19,2.67', shape=(), dtype=string)\n","tf.Tensor(b'2.1856,41.0,3.7189873417721517,1.0658227848101265,803.0,2.0329113924050635,32.76,-117.12,1.205', shape=(), dtype=string)\n","tf.Tensor(b'4.1812,52.0,5.701388888888889,0.9965277777777778,692.0,2.4027777777777777,33.73,-118.31,3.215', shape=(), dtype=string)\n","```\n","\\\n","\\\n","**데이터 전처리와 적재 합치기**\n","- list_file -> interleave -> map(preprocess) -> shuffle -> batch -> prepatch\n","\n","```\n","앞에서 만든 데이터(적재한 데이터)들을 표준화 전처리와 함께 묶은 예시\n","from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","scaler.fit(X_train)\n","\n","X_mean, X_std = scaler.mean_, scaler.scale_  # 추가 코드\n","n_inputs = 8\n","\n","def parse_csv_line(line):\n","    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n","    fields = tf.io.decode_csv(line, record_defaults=defs)\n","    return tf.stack(fields[:-1]), tf.stack(fields[-1:])\n","\n","def preprocess(line):\n","    x, y = parse_csv_line(line)\n","    return (x - X_mean) / X_std, y\n","\n","\n","def csv_reader_dataset(filepaths, n_readers=5, n_read_threads=None,\n","                       n_parse_threads=5, shuffle_buffer_size=10_000, seed=42,\n","                       batch_size=32):\n","    dataset = tf.data.Dataset.list_files(filepaths, seed=seed)\n","    dataset = dataset.interleave(\n","        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n","        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n","    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n","    dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)\n","    return dataset.batch(batch_size).prefetch(1) #cpu가 다음 배치 준비\n","```\n","```\n","그럼 이제\n","train_set = csv_reader_dataset(train_filepaths)\n","valid_set = csv_reader_dataset(valid_filepaths)\n","test_set = csv_reader_dataset(test_filepaths)\n","\n","X_train,y_train 대신 train_set를\n","X_valid, y_valid 대신 valid_set를\n","X_test, y_test 대신 test_set를 사용할 수 있다.\n","\n","-> 이런 방식을 tf.data pipeline에서 전처리하는 방식이라고 한다.\n","```"],"metadata":{"id":"HYPmtJnFDnqs"}},{"cell_type":"code","source":["\"\"\"\n","tf.data.Dataset 클래스 메소드 정리\n","* : 저자 추천\n","* apply(): 이 데이터셋에 변환 함수를 적용합니다.\n","as_numpy_iterator(): 데이터셋의 모든 요소를 넘파이(numpy)로 변환하는 반복자를 반환합니다.\n","batch(): 이 데이터셋의 연속적인 요소들을 배치로 결합합니다.\n","bucket_by_sequence_length(): 데이터셋의 요소를 길이에 따라 분류하는 변환을 수행합니다.\n","cache(): 이 데이터셋의 요소들을 캐시합니다.\n","cardinality(): 데이터셋의 카디널리티(요소의 수)를 반환합니다(알려진 경우).\n","choose_from_datasets(): datasets에서 요소를 결정적으로 선택하는 데이터셋을 생성합니다.\n","* concatenate(): 주어진 데이터셋을 이 데이터셋과 연결하여 새로운 Dataset을 생성합니다.\n","counter(): start부터 step 크기의 단계로 셈하는 Dataset을 생성합니다.\n","element_spec(): 이 데이터셋의 요소의 타입 명세를 반환합니다.\n","enumerate(): 이 데이터셋의 요소들을 열거합니다.\n","filter(): predicate에 따라 이 데이터셋을 필터링합니다.\n","* flat_map(): 이 데이터셋에 map_func을 매핑하고 결과를 평탄화합니다.\n","* from_generator(): generator에 의해 생성된 요소들로 Dataset을 생성합니다. (deprecated arguments)\n","from_tensor_slices(): 주어진 텐서의 슬라이스로 Dataset을 생성합니다.\n","* from_tensors(): 주어진 텐서들로 구성된 단일 요소를 가진 Dataset을 생성합니다.\n","get_single_element(): dataset의 단일 요소를 반환합니다.\n","group_by_window(): 키에 따라 요소의 윈도우를 그룹화하고 그것들을 축소합니다.\n","ignore_errors(): 오류를 유발하는 요소들을 무시합니다.\n","interleave(): 이 데이터셋에 map_func을 매핑하고 결과를 교차 배열합니다.\n","list_files(): 하나 이상의 글로브 패턴에 일치하는 모든 파일의 데이터셋입니다.\n","load(): 이전에 저장된 데이터셋을 불러옵니다.\n","map(): 이 데이터셋의 요소들에 map_func을 매핑합니다.\n","options(): 이 데이터셋과 그 입력의 옵션을 반환합니다.\n","* padded_batch(): 이 데이터셋의 연속적인 요소들을 패딩된 배치로 결합합니다.\n","* prefetch(): 이 데이터셋에서 요소들을 미리 가져오는 Dataset을 생성합니다.\n","ragged_batch(): 이 데이터셋의 연속적인 요소들을 tf.RaggedTensor들로 결합합니다.\n","random(): 의사난수 값을 가진 Dataset을 생성합니다.\n","range(): 값의 범위에 대한 Dataset을 생성합니다.\n","rebatch(): 이 데이터셋의 요소들을 재배치하는 Dataset을 생성합니다.\n","* reduce(): 입력 데이터셋을 단일 요소로 축소합니다.\n","rejection_resample(): 목표 분포에 도달하기 위해 요소들을 재샘플링합니다.\n","repeat(): 이 데이터셋을 반복하여 각 원본 값을 count 번씩 보여줍니다.\n","sample_from_datasets(): datasets의 데이터셋에서 무작위로 요소를 샘플링합니다.\n","save(): 주어진 데이터셋의 내용을 저장합니다.\n","scan(): 입력 데이터셋에 대해 함수를 스캔하는 변환입니다.\n","* shard(): 이 데이터셋의 1/num_shards만 포함하는 Dataset을 생성합니다.\n","shuffle(): 이 데이터셋의 요소들을 무작위로 섞습니다.\n","skip(): 이 데이터셋에서 count 요소를 건너뛰는 Dataset을 생성합니다.\n","snapshot(): 입력 데이터셋의 출력을 유지하기 위한 API입니다.\n","sparse_batch(): 연속적인 요소들을 tf.sparse.SparseTensor들로 결합합니다.\n","take(): 이 데이터셋에서 최대 count 요소를 가진 Dataset을 생성합니다.\n","take_while(): predicate에 따라 데이터셋 반복을 중단하는 변환입니다.\n","unbatch(): 데이터셋의 요소들을 여러 요소로 분할합니다.\n","unique(): Dataset의 중복 요소를 제거하는 변환입니다.\n","* window(): \"윈도우\"의 데이터셋을 반환합니다.\n","with_options(): 주어진 옵션을 설정한 새로운 tf.data.Dataset을 반환합니다.\n","* zip(): 주어진 데이터셋들을 함께 묶어 Dataset을 생성합니다.\n","\"\"\""],"metadata":{"id":"LhPyzkmWPqAu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","중요 함수 요약\n","apply()\n","기능: 사용자 정의 또는 미리 정의된 변환 함수를 데이터셋에 적용합니다.\n","사용 예: 데이터셋의 구조를 변경하거나, 복잡한 변환을 데이터셋에 적용할 때 유용합니다.\n","\n","concatenate()\n","기능: 두 데이터셋을 순차적으로 연결합니다. 첫 번째 데이터셋 뒤에 두 번째 데이터셋이 이어집니다.\n","사용 예: 서로 다른 두 데이터셋을 하나의 연속된 데이터셋으로 병합할 때 사용됩니다.\n","\n","flat_map()\n","기능: 각 요소에 함수를 적용하고, 그 결과를 평탄화하여 하나의 데이터셋으로 반환합니다.\n","사용 예: 각 요소가 여러 개의 하위 요소를 포함하는 경우, 이를 단일 데이터셋으로 평탄화하는 데 사용됩니다.\n","\n","from_generator()\n","기능: 파이썬 제너레이터(generator) 함수로부터 데이터셋을 생성합니다.\n","사용 예: 파이썬의 반복 가능한 객체나 함수를 통해 데이터를 생성할 때 사용됩니다.\n","\n","from_tensors()\n","기능: 텐서들로부터 단일 요소를 가진 데이터셋을 생성합니다.\n","사용 예: 고정된 텐서 데이터를 데이터셋으로 변환할 때 사용됩니다.\n","\n","padded_batch()\n","기능: 배치로 결합된 요소들을 패딩하여 모든 요소가 같은 크기를 갖도록 합니다.\n","사용 예: 길이가 다른 시퀀스 데이터를 동일한 길이로 패딩하여 배치 처리할 때 사용됩니다.\n","\n","prefetch()\n","기능: 데이터 처리 및 모델 학습 파이프라인의 효율성을 높이기 위해 데이터셋의 요소들을 미리 가져옵니다.\n","사용 예: 학습 중에 데이터 로딩 시간을 줄이고 GPU와 같은 계산 자원의 활용도를 높일 때 사용됩니다.\n","\n","reduce()\n","기능: 데이터셋을 단일 요소로 축소합니다. 주어진 리듀스 함수에 따라 모든 요소를 결합합니다.\n","사용 예: 데이터셋의 모든 요소를 합치거나 평균을 내는 등의 집계 연산에 사용됩니다.\n","\n","shard()\n","기능: 전체 데이터셋을 여러 개의 샤드로 분할하고, 지정된 샤드만 포함하는 데이터셋을 생성합니다.\n","사용 예: 분산 학습 환경에서 데이터셋을 여러 작업자 간에 나눌 때 사용됩니다.\n","\n","window()\n","기능: 데이터셋을 여러 개의 작은 '윈도우' 데이터셋으로 분할합니다.\n","사용 예: 시계열 데이터나 연속적인 데이터 시퀀스를 처리할 때 각 시퀀스를 작은 부분으로 나누는 데 사용됩니다.\n","\n","zip()\n","기능: 둘 이상의 데이터셋을 병렬적으로 결합하여, 요소들이 튜플 형태로 묶인 새로운 데이터셋을 생성합니다.\n","사용 예: 서로 다른 특성을 가진 여러 데이터셋을 동시에 사용할 때 사용됩니다. 예를 들어, 이미지 데이터셋과 해당 이미지의 레이블 데이터셋을 결합할 때 유용합니다.\n","\"\"\""],"metadata":{"id":"hSgN_m-fQ-ZA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **TFTRecord**"],"metadata":{"id":"Yp_KVtRQSVCo"}},{"cell_type":"code","source":["\"\"\"\n","TFTRecord는 이진 레코드 목록으로, 대용량 데이터를 저장하고 효율적으로 읽기 위해\n","텐서플로에서 선호하는 포맷이다. CSV나 다른 포맷에서 병목이 생기거나 문제가 없다면\n","TFTRecord를 사용하지 않아도 된다.\n","\"\"\"\n","import tensorflow as tf\n","\n","#아래와 같이 파일을 만들 수 있다.\n","with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n","    f.write(b\"This is the first record\")\n","    f.write(b\"And this is the second record\")\n","\n","#파일은 이렇게 불러온다.\n","filepaths = [\"my_data.tfrecord\"]\n","dataset = tf.data.TFRecordDataset(filepaths)\n","for item in dataset:\n","    print(item)"],"metadata":{"id":"W3ENTWDfSZsk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#파일을 압축하거나, 압축한 파일을 불러오려면 아래와 같이 해야한다.\n","\n","#압축 형식을 지정하고, 파일을 만든다.\n","options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n","with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n","    f.write(b\"Compress, compress, compress!\")\n","\n","#불러올때는 압축 형식을 지정하고 불러온다.\n","dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\n","                                  compression_type=\"GZIP\")\n","for item in dataset:\n","    print(item)"],"metadata":{"id":"cKjMCRGlTFof"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **전처리** (number, string, image)\n"],"metadata":{"id":"6NIYiRMhXQWj"}},{"cell_type":"markdown","source":["**1. Normlization (수치 데이터)**\n","```\n","#훈련 전에 데이터에 전처리 처리\n","norm_layer = tf.keras.layers.Normalization()\n","nrom_layer.adapt(X_train)\n","X_train_scaled = norm_layer(X_train)\n","X_valid_scaled = norm_layer(X_valid)\n","\n","# 모델 훈련\n","model = keras.model.Sequential([layers.Dense(1)])\n","model.complie = [...]\n","model.fit(X_train_scaled, y_train, epoch = 5,\n","          validation_data = (X_valid_scaled, y_valid))\n","\n","# 최종 모델 테스트\n","final_model = keras.Sequential([norm_layer, model])\n","X_new = X_test[:3]\n","y_pred = final_model(X_new)\n","```\n","\n","**2. Discretization (수치 데이터)**   \n","값을 구간 별로 나누는것 (20대, 30대, 40대 ...)\n","```\n","age = tf.constant([[10.], [93.], [57.], [18.], [37.], [5.]]).\n","\n","# 18보다 작은 값, 18~50 값, 50~ 값이 0,1,2로 인코딩 된다.\n","discretize_layer = tf.keras.layers.Discretization(bin_boundaries=[18., 50.])\n","age_categories = discretize_layer(age)\n","-> array([[0],\n","       [2],\n","       [2],\n","       [1],\n","       [1],\n","       [0]])>\n","\n","#아니면 이렇게 원하는 구간 개수를 전달할 수도 있다.\n","discretize_layer = tf.keras.layers.Discretization(num_bins=3)\n","discretize_layer.adapt(age)\n","age_categories = discretize_layer(age)\n","->array([[1],\n","       [2],\n","       [2],\n","       [1],\n","       [2],\n","       [0]])>\n","\n","#그러나 이렇게 구간별로 나눈다음 써먹으려면 원핫인코딩으로 변환해야 한다.\n","onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3)\n","onehot_layer(age_categories)\n","->array([[0., 1., 0.],\n","       [0., 0., 1.],\n","       [0., 0., 1.],\n","       [0., 1., 0.],\n","       [0., 0., 1.],\n","       [1., 0., 0.]], dtype=float32)>\n","\n","#이렇게 쓰면 멀티-핫 인코딩을 수행할 수 있으며, 원-핫 인코딩과\n","멀티-핫 인코딩중 뭐가 더 나을지는 해봐야 안다.\n","two_age_categories = np.array([[1, 0], [2, 2], [2, 0]])\n","onehot_layer(two_age_categories)\n","```\n","\n","**3.StringLookup (문자 데이터)**\n","```\n","#output_mode = \"one_hot\"으로 지정 안하면 정수 인덱스를 반환함.\n","str_lookup_layer = tf.keras.layers.StringLookup(output_mode=\"one_hot\")\n","str_lookup_layer.adapt(cities)\n","str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Auckland\"], [\"Montreal\"]])\n","->array([[0., 1., 0., 0.],\n","       [0., 0., 0., 1.],\n","       [0., 0., 0., 1.],\n","       [1., 0., 0., 0.]], dtype=float32)>\n","```\n","\n","**4.Embedding (문자 데이터)**\n","```\n","만약 특성이 수 만개가 넘어가면 이걸 모두 원-핫 인코딩 하는것은 불가능하다.\n","50000개의 특성(차원)이 있을때 이를 원-핫 인코딩으로 표현하려면   \n","50000차원의 희소 벡터가 필요할 것이다. 그러나 임베딩을 활용하면\n","100차원의 밀집 벡터로 표현할 수 있다.\n","import tensorflow as tf\n","\n","#문자열을 정수로 변환한 뒤, 임베딩하기.\n","ocean_prox = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n","str_lookup_layer = tf.keras.layers.StringLookup()\n","str_lookup_layer.adapt(ocean_prox)\n","lookup_and_embed = tf.keras.Sequential([\n","    tf.keras.layers.InputLayer(input_shape=[], dtype=tf.string),  # WORKAROUND\n","    str_lookup_layer,\n","    tf.keras.layers.Embedding(input_dim=str_lookup_layer.vocabulary_size(),\n","                             output_dim=2)\n","])\n","\n","lookup_and_embed(np.array([\"<1H OCEAN\", \"ISLAND\", \"<1H OCEAN\"]))\n","-> array([[-0.03449249,  0.01491299],\n","       [ 0.02290317, -0.03581462],\n","       [-0.03449249,  0.01491299]], dtype=float32)>\n","```\n","\\\n","\n","```\n","수치 특성과 문자 특성을 모두 전처리하는 예시\n","tf.random.set_seed(42)\n","np.random.seed(42)\n","X_train_num = np.random.rand(10_000, 8)\n","X_train_cat = np.random.choice(ocean_prox, size=10_000)\n","y_train = np.random.rand(10_000, 1)\n","X_valid_num = np.random.rand(2_000, 8)\n","X_valid_cat = np.random.choice(ocean_prox, size=2_000)\n","y_valid = np.random.rand(2_000, 1)\n","\n","num_input = tf.keras.layers.Input(shape=[8], name=\"num\")\n","cat_input = tf.keras.layers.Input(shape=[], dtype=tf.string, name=\"cat\")\n","cat_embeddings = lookup_and_embed(cat_input)\n","encoded_inputs = tf.keras.layers.concatenate([num_input, cat_embeddings])\n","outputs = tf.keras.layers.Dense(1)(encoded_inputs)\n","model = tf.keras.models.Model(inputs=[num_input, cat_input], outputs=[outputs])\n","model.compile(loss=\"mse\", optimizer=\"sgd\")\n","history = model.fit((X_train_num, X_train_cat), y_train, epochs=5,\n","                    validation_data=((X_valid_num, X_valid_cat), y_valid))\n","```\n","\n","**5. TextVectorization (텍스트 전처리)**\n","```\n","standardize = None (대소문자, 구두점 유지)\n","output_sequence_length = N (원하는 길이로 문자 자르거나 패딩)\n","\n","\n","#사용법은 StringLookup과 유사하다.\n","train_data = [\"To be\", \"!(to be)\", \"That's the question\", \"Be, be, be.\"]\n","text_vec_layer = tf.keras.layers.TextVectorization()\n","text_vec_layer.adapt(train_data)\n","text_vec_layer([\"Be good!\", \"Question: be or be?\"])\n","-> array([[2, 1, 0, 0],\n","       [6, 2, 1, 2]])>\n","\n","# TextVectorization에는 다양한 옵션이 있으며 아래와 같은\n","tf_idf는 많이 등장하는 단어(to, be, is)등에는 낮은 가중치를 부여하고\n","드물게 등장하는 단어에는 높은 가중치를 부여한다.\n","\n","text_vec_layer = tf.keras.layers.TextVectorization(output_mode=\"tf_idf\")\n","text_vec_layer.adapt(train_data)\n","text_vec_layer([\"Be good!\", \"Question: be or be?\"])\n","\n","#마지막으로 tensorflow hub나 허깅페이스 허브 등에서 사전 훈련된 모델을\n","다운로드 받아서 사용할 수 있다.\n","import tensorflow_hub as hub\n","\n","hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim50/2\")\n","sentence_embeddings = hub_layer(tf.constant([\"To be\", \"Not to be\"]))\n","sentence_embeddings.numpy().round(2)\n","```\n","\n","**6. 이미지 전처리**\n","1. Resizing (이미지 크기 변경, 비율 고정 지원)\n","2. Rescaling (픽셀 스케일 조정)\n","3. CenterCrop (주위 이미지 Crop)\n","```\n","image = [이미지를 불러온다...]\n","crop_image_layer = tf.keras.layers.CaenterCrop(height = 100, width = 100)\n","cropped_images = crop_image_layer(images)\n","```"],"metadata":{"id":"GP7vRlZpX0HS"}}]}